# DAG'и для работы с rabota.by

## Обзор

Три DAG'а для работы с вакансиями с rabota.by:

1. **rabota_fetch_new_vacancies** - Загрузка новых вакансий
2. **rabota_update_existing_vacancies** - Обновление существующих вакансий
3. **rabota_archive_old_vacancies** - Архивирование старых вакансий

## Зависимости

Для работы DAG'ов требуется установить следующие Python-пакеты в окружении Airflow:

```bash
pip install beautifulsoup4 lxml requests
```

Или добавить в `requirements.txt` для Airflow:

```
beautifulsoup4>=4.12.0
lxml>=4.9.0
requests>=2.31.0
```

## Настройка

### Airflow Variables

Создайте следующие переменные в Airflow:

- `rabota_search_text` - Текст для поиска вакансий (по умолчанию: "Python OR Java OR JavaScript")
- `rabota_search_area` - ID региона для поиска (по умолчанию: "1000" - Минск)
- `rabota_update_batch_size` - Количество вакансий для обновления за один запуск (по умолчанию: 100)

### Структура данных

DAG'и работают с обновленной схемой БД, которая включает:

- Полные описания: `tasks`, `requirements`, `advantages`, `offers`
- Справочники: регионы, специализации, отрасли, метро
- Связи: вакансии-метро, вакансии-специализации, вакансии-навыки

## Особенности реализации

### Парсинг данных

Поскольку rabota.by может не иметь публичного API, DAG'и используют парсинг HTML:

1. **Поиск вакансий**: Парсинг страницы поиска для получения списка вакансий
2. **Детальная информация**: Парсинг страницы каждой вакансии для получения полных данных

### Адаптация под реальную структуру

⚠️ **Важно**: Текущая реализация содержит базовую структуру парсинга, которую необходимо адаптировать под реальную структуру страниц rabota.by.

Для адаптации:

1. Изучите HTML-структуру страниц rabota.by
2. Обновите селекторы в функциях `parse_vacancy_page()`
3. Проверьте формат JSON данных, если они встроены в страницу
4. Адаптируйте извлечение данных под реальные классы и структуру

### Пример адаптации

```python
# В функции parse_vacancy_page() обновите селекторы:
description_block = soup.find('div', class_='vacancy-description')  # Замените на реальный класс
tasks_block = soup.find('div', {'data-section': 'tasks'})  # Адаптируйте под реальную структуру
```

## Расписание

- **rabota_fetch_new_vacancies**: Каждые 6 часов
- **rabota_update_existing_vacancies**: Каждые 12 часов
- **rabota_archive_old_vacancies**: Раз в день

## Логирование

Все DAG'и логируют:
- Количество обработанных вакансий
- Ошибки при парсинге
- Статистику обновлений

Статистика сохраняется в XCom для последующего анализа.

## Обработка ошибок

DAG'и обрабатывают следующие ошибки:
- Ошибки сети (retry с задержкой)
- Ошибки парсинга (логирование и пропуск вакансии)
- Ошибки БД (логирование и продолжение)

## Следующие шаги

1. Изучить реальную структуру страниц rabota.by
2. Адаптировать парсинг под реальные селекторы
3. Протестировать на небольшом количестве вакансий
4. Настроить rate limiting для избежания блокировок
5. Добавить обработку капчи, если требуется

