"""
DAG –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å rabota.by
–ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –Ω–∞–≤—ã–∫–æ–≤
–ê–≤—Ç–æ—Ä: kiselevas
–î–∞—Ç–∞: 2025-12-07
"""

from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable
import requests
import logging
import time
import json
import re
import html as html_lib
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from dateutil import parser as date_parser

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Rabota.by
RABOTA_BASE_URL = "https://rabota.by"
RABOTA_SEARCH_URL = f"{RABOTA_BASE_URL}/search/vacancy"
RABOTA_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

# –ú–µ—Å—è—Ü—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä—É—Å—Å–∫–∏—Ö –¥–∞—Ç
MONTHS_RU = {
    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
    '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
    '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
}

# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –¥–ª—è fallback –ø–æ–∏—Å–∫–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
TECH_KEYWORDS = [
    # –Ø–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
    'Python', 'Java', 'JavaScript', 'TypeScript', 'PHP', 'C#', 'C++', 'Go', 'Golang', 
    'Ruby', 'Kotlin', 'Swift', 'Rust', 'Scala', 'Perl', 'Delphi', 'Pascal', 'VB.NET',
    
    # –§—Ä–µ–π–º–≤–æ—Ä–∫–∏ –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
    'React', 'Angular', 'Vue', 'Vue.js', 'Node.js', 'Express', 'Django', 'Flask', 'FastAPI',
    'Spring', 'Spring Boot', 'Laravel', 'Symfony', 'Yii', 'Rails', '.NET', 'ASP.NET',
    
    # –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    'SQL', 'NoSQL', 'MongoDB', 'PostgreSQL', 'MySQL', 'Redis', 'Elasticsearch', 
    'Oracle', 'MS SQL', 'Cassandra', 'DynamoDB', 'MariaDB', 'SQLite',
    
    # –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ DevOps
    'Docker', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'Jenkins', 'GitLab', 'CI/CD',
    'Terraform', 'Ansible', 'Linux', 'Nginx', 'Apache', 'RabbitMQ', 'Kafka',
    
    # –î—Ä—É–≥–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    'Git', 'REST', 'API', 'GraphQL', 'WebSocket', 'gRPC', 'Microservices',
    'HTML', 'CSS', 'SASS', 'Bootstrap', 'Tailwind', 'Material-UI',
    'Unit Testing', 'TDD', 'BDD', 'Agile', 'Scrum', 'SOLID', 'Design Patterns'
]

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
default_args = {
    'owner': 'kiselevas',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 27),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}


def get_rabota_session():
    """–°–æ–∑–¥–∞—ë—Ç HTTP —Å–µ—Å—Å–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –¥–ª—è Rabota.by"""
    session = requests.Session()
    session.headers.update({
        'User-Agent': RABOTA_USER_AGENT,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,be;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Referer': 'https://rabota.by/',
        'Upgrade-Insecure-Requests': '1',
        'Cache-Control': 'max-age=0'
    })
    return session


def parse_russian_date(date_text: str) -> datetime:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ä—É—Å—Å–∫–∏—Ö –¥–∞—Ç —Ç–∏–ø–∞ '30 –Ω–æ—è–±—Ä—è 2024'"""
    try:
        if not date_text:
            return datetime.now()
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–π —Ç–µ–∫—Å—Ç
        date_text = re.sub(
            r'(–†–∞–∑–º–µ—â–µ–Ω–æ|–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ|–≤–∞–∫–∞–Ω—Å–∏—è —Ä–∞–∑–º–µ—â–µ–Ω–∞|–æ–±–Ω–æ–≤–ª–µ–Ω–∞)\s*',
            '', 
            date_text, 
            flags=re.I
        ).strip()
        
        # –ï—Å–ª–∏ ISO —Ñ–æ—Ä–º–∞—Ç
        if 'T' in date_text:
            return date_parser.parse(date_text)
        
        # –û—Å–æ–±—ã–µ —Å–ª—É—á–∞–∏
        if '—Å–µ–≥–æ–¥–Ω—è' in date_text.lower():
            return datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        elif '–≤—á–µ—Ä–∞' in date_text.lower():
            return (datetime.now() - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        
        # –ü–∞—Ä—Å–∏–º —Ä—É—Å—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç "30 –Ω–æ—è–±—Ä—è 2024" –∏–ª–∏ "30 –Ω–æ—è–±—Ä—è"
        for month_ru, month_num in MONTHS_RU.items():
            if month_ru in date_text.lower():
                # –ò—â–µ–º –¥–µ–Ω—å
                day_match = re.search(r'(\d{1,2})', date_text)
                if day_match:
                    day = int(day_match.group(1))
                    
                    # –ò—â–µ–º –≥–æ–¥
                    year_match = re.search(r'(\d{4})', date_text)
                    if year_match:
                        year = int(year_match.group(1))
                    else:
                        # –ï—Å–ª–∏ –Ω–µ—Ç –≥–æ–¥–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π
                        year = datetime.now().year
                    
                    try:
                        return datetime(year, month_num, day)
                    except ValueError:
                        pass
                break
        
        # –ü—Ä–æ–±—É–µ–º dateutil parser –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
        return date_parser.parse(date_text, dayfirst=True)
        
    except Exception as e:
        logger.debug(f"Could not parse date '{date_text}': {e}")
        return datetime.now()


def extract_json_ld(soup: BeautifulSoup) -> Optional[Dict]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö JSON-LD"""
    json_ld_scripts = soup.find_all('script', type='application/ld+json')
    for script in json_ld_scripts:
        try:
            data = json.loads(script.string)
            if isinstance(data, list):
                for item in data:
                    if item.get('@type') == 'JobPosting':
                        return item
            elif data.get('@type') == 'JobPosting':
                return data
        except (json.JSONDecodeError, AttributeError):
            continue
    return None


def extract_vacancy_data(html: str) -> Optional[Dict]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö JSON —Å—Ç—Ä—É–∫—Ç—É—Ä –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    """
    patterns = [
        r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
        r'HH\.VacancyResponsePage\.init\(({.+?})\);',
        r'HH\.globalVacancyData\s*=\s*({.+?});',
        r'"vacancy":\s*({.+?}),\s*"',
        r'data-vacancy-json="([^"]+)"',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, html, re.DOTALL)
        if match:
            try:
                # –ï—Å–ª–∏ —ç—Ç–æ –∞—Ç—Ä–∏–±—É—Ç HTML, –Ω—É–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
                if 'data-vacancy-json' in pattern:
                    json_str = html_lib.unescape(match.group(1))
                else:
                    json_str = match.group(1)
                
                data = json.loads(json_str)
                return data
            except (json.JSONDecodeError, AttributeError) as e:
                logger.debug(f"Failed to parse JSON with pattern {pattern}: {e}")
                continue
    
    return None


def extract_key_skills_from_page(html: str) -> List[str]:
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    key_skills = []
    
    # –ú–µ—Ç–æ–¥ 1: –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≤ HTML —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–∫–∏
    patterns = [
        r'"keySkills":\s*{[^}]*"keySkill":\s*\[([^\]]+)\]',
        r'"keySkill":\s*\[([^\]]+)\]',
        r'"key_skills":\s*\[([^\]]+)\]',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, html, re.DOTALL)
        if match:
            try:
                skills_str = match.group(1)
                skills = re.findall(r'"([^"]+)"', skills_str)
                key_skills.extend(skills)
                if key_skills:
                    logger.info(f"‚úÖ Found {len(key_skills)} key skills from regex pattern")
                    return key_skills
            except Exception as e:
                logger.debug(f"Error extracting skills from pattern {pattern}: {e}")
    
    # –ú–µ—Ç–æ–¥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ–ª–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç
    vacancy_data = extract_vacancy_data(html)
    if vacancy_data:
        # –†–∞–∑–ª–∏—á–Ω—ã–µ –ø—É—Ç–∏ –∫ –Ω–∞–≤—ã–∫–∞–º –≤ JSON
        paths = [
            ['keySkills', 'keySkill'],
            ['key_skills'],
            ['skills'],
            ['vacancy', 'keySkills', 'keySkill'],
            ['vacancy', 'key_skills'],
        ]
        
        for path in paths:
            try:
                current = vacancy_data
                for key in path:
                    if isinstance(current, dict) and key in current:
                        current = current[key]
                    else:
                        break
                else:
                    # –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à–ª–∏ –≤–µ—Å—å –ø—É—Ç—å
                    if isinstance(current, list):
                        key_skills = [str(s) for s in current if s]
                        if key_skills:
                            logger.info(f"‚úÖ Found {len(key_skills)} key skills from JSON path: {' > '.join(path)}")
                            return key_skills
            except Exception as e:
                logger.debug(f"Error extracting skills from path {path}: {e}")
    
    # –ú–µ—Ç–æ–¥ 3: –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ HTML –±–ª–æ–∫–æ–≤ (fallback)
    if not key_skills:
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º –±–ª–æ–∫ —Å –Ω–∞–≤—ã–∫–∞–º–∏
        skills_selectors = [
            'div[data-qa="skills-element"] span[data-qa="bloko-tag__text"]',
            'div.bloko-tag-list span.bloko-tag__section_text',
            'div[class*="skill"] span',
            'span.bloko-tag__text'
        ]
        
        for selector in skills_selectors:
            skill_elements = soup.select(selector)
            if skill_elements:
                for elem in skill_elements:
                    skill_text = elem.get_text(strip=True)
                    if skill_text and len(skill_text) > 1:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä
                        if not any(x in skill_text.lower() for x in ['–Ω–∞–≤—ã–∫', '–æ–ø—ã—Ç', '—Ç—Ä–µ–±–æ–≤–∞–Ω', '–Ω–µ–æ–±—Ö–æ–¥–∏–º']):
                            key_skills.append(skill_text)
                
                if key_skills:
                    logger.info(f"‚úÖ Found {len(key_skills)} key skills from HTML selectors")
                    break
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    seen = set()
    unique_skills = []
    for skill in key_skills:
        if skill not in seen:
            seen.add(skill)
            unique_skills.append(skill)
    
    return unique_skills


def extract_skills_from_description(description: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏ (fallback –º–µ—Ç–æ–¥)
    """
    if not description:
        return []
    
    found_techs = []
    desc_lower = description.lower()
    
    for tech in TECH_KEYWORDS:
        # –ò—â–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é —Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤
        pattern = r'\b' + re.escape(tech.lower()) + r'\b'
        if re.search(pattern, desc_lower):
            found_techs.append(tech)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    return list(dict.fromkeys(found_techs))


def parse_complex_address(address_text: str) -> Dict[str, Any]:
    """
    –£–º–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ª–æ–∂–Ω–æ–≥–æ –∞–¥—Ä–µ—Å–∞
    –ü—Ä–∏–º–µ—Ä: '–ú–∏–Ω—Å–∫,–ú–æ–ª–æ–¥–µ–∂–Ω–∞—è,–ü–ª–æ—â–∞–¥—å –§—Ä–∞–Ω—Ç–∏—à–∫–∞ –ë–æ–≥—É—à–µ–≤–∏—á–∞,–§—Ä—É–Ω–∑–µ–Ω—Å–∫–∞—è,–Æ–±–∏–ª–µ–π–Ω–∞—è –ø–ª–æ—â–∞–¥—å, —É–ª–∏—Ü–∞ –¢–∏–º–∏—Ä—è–∑–µ–≤–∞, 9–∫10'
    –õ–æ–≥–∏–∫–∞: –≥–æ—Ä–æ–¥ —Å–ª–µ–≤–∞, –∑–¥–∞–Ω–∏–µ –∏ —É–ª–∏—Ü–∞ —Å–ø—Ä–∞–≤–∞, –≤—Å—ë –º–µ–∂–¥—É –Ω–∏–º–∏ - –º–µ—Ç—Ä–æ
    """
    result = {
        'city': None,
        'street': None,
        'building': None,
        'metro_stations': []
    }
    
    if not address_text:
        return result
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –∞–¥—Ä–µ—Å –Ω–∞ —á–∞—Å—Ç–∏
    parts = [p.strip() for p in re.split(r'[,;]', address_text) if p.strip()]
    
    if not parts:
        return result
    
    # 1. –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å - –æ–±—ã—á–Ω–æ –≥–æ—Ä–æ–¥
    city_keywords = ['–ú–∏–Ω—Å–∫', '–ë—Ä–µ—Å—Ç', '–í–∏—Ç–µ–±—Å–∫', '–ì–æ–º–µ–ª—å', '–ì—Ä–æ–¥–Ω–æ', '–ú–æ–≥–∏–ª–µ–≤', '–ë–æ–±—Ä—É–π—Å–∫', '–ë–∞—Ä–∞–Ω–æ–≤–∏—á–∏', '–ü–∏–Ω—Å–∫']
    first_part = parts[0]
    if any(keyword in first_part for keyword in city_keywords):
        result['city'] = first_part
        parts = parts[1:]  # –£–±–∏—Ä–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ —Å–ø–∏—Å–∫–∞
    
    if not parts:
        return result
    
    # 2. –ò–¥—ë–º —Å –∫–æ–Ω—Ü–∞ –∏ –∏—â–µ–º –∑–¥–∞–Ω–∏–µ –∏ —É–ª–∏—Ü—É
    # –ü–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–æ–º–µ—Ä –¥–æ–º–∞
    last_part = parts[-1] if parts else None
    if last_part:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –Ω–æ–º–µ—Ä –¥–æ–º–∞/–∫–æ—Ä–ø—É—Å–∞
        if re.search(r'\d+[–∞-—èa-z]?\d*|–∫\d+|–∫–æ—Ä–ø—É—Å\s*\d+|—Å—Ç—Ä\s*\d+|–¥\.\s*\d+', last_part, re.IGNORECASE):
            result['building'] = last_part
            parts = parts[:-1]  # –£–±–∏—Ä–∞–µ–º –∑–¥–∞–Ω–∏–µ
    
    # –ü—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç (–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –∑–¥–∞–Ω–∏—è) - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —É–ª–∏—Ü—É
    if parts:
        last_part = parts[-1]
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —É–ª–∏—Ü–µ–π
        street_keywords = ['—É–ª–∏—Ü–∞', '—É–ª.', '–ø—Ä–æ—Å–ø–µ–∫—Ç', '–ø—Ä.', '–ø—Ä-—Ç', '–ø–µ—Ä–µ—É–ª–æ–∫', '–ø–µ—Ä.', 
                         '–ø–ª–æ—â–∞–¥—å', '–ø–ª.', '–±—É–ª—å–≤–∞—Ä', '–±-—Ä', '–ø—Ä–æ–µ–∑–¥', '—à–æ—Å—Å–µ', 
                         '–Ω–∞–±–µ—Ä–µ–∂–Ω–∞—è', '–Ω–∞–±.', '—Ç—Ä–∞–∫—Ç']
        if any(keyword in last_part.lower() for keyword in street_keywords):
            result['street'] = last_part
            parts = parts[:-1]  # –£–±–∏—Ä–∞–µ–º —É–ª–∏—Ü—É
    
    # 3. –í—Å—ë —á—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å –º–µ–∂–¥—É –≥–æ—Ä–æ–¥–æ–º –∏ —É–ª–∏—Ü–µ–π/–¥–æ–º–æ–º - —Å—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ
    for part in parts:
        # –û—á–∏—â–∞–µ–º –æ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –º–µ—Ç—Ä–æ
        clean_station = re.sub(r'(?:^|\s)(?:–º\.|–º–µ—Ç—Ä–æ|—Å—Ç\.–º\.|—Å—Ç–∞–Ω—Ü–∏—è –º–µ—Ç—Ä–æ)\s*', '', part, flags=re.IGNORECASE)
        clean_station = clean_station.strip()
        
        if clean_station:
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ —Å—Ç–∞–Ω—Ü–∏—é –º–µ—Ç—Ä–æ
            result['metro_stations'].append(clean_station)
    
    return result


def find_metro_station_id(pg_hook, station_name: str) -> Optional[int]:
    """
    –ü–æ–∏—Å–∫ —Å—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ metro_stations_by
    —Å –Ω–µ—á—ë—Ç–∫–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –Ω–∞–∑–≤–∞–Ω–∏–π
    """
    if not station_name or len(station_name.strip()) < 2:
        return None
    
    station_name = station_name.strip()
    
    # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–±–µ–∑ —É—á—ë—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)
    result = pg_hook.get_first(
        """
        SELECT id FROM vacancies.metro_stations_by 
        WHERE LOWER(TRIM(name)) = LOWER(TRIM(%s))
        """,
        parameters=(station_name,)
    )
    if result:
        return result[0]
    
    # 2. –ù–∞–∑–≤–∞–Ω–∏–µ –≤ –ë–î –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∏—Å–∫–æ–º–æ–≥–æ
    result = pg_hook.get_first(
        """
        SELECT id FROM vacancies.metro_stations_by 
        WHERE LOWER(name) LIKE LOWER(%s) || '%%'
        ORDER BY LENGTH(name)
        LIMIT 1
        """,
        parameters=(station_name,)
    )
    if result:
        return result[0]
    
    # 3. –ò—Å–∫–æ–º–æ–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏–∑ –ë–î
    result = pg_hook.get_first(
        """
        SELECT id FROM vacancies.metro_stations_by 
        WHERE LOWER(name) LIKE '%%' || LOWER(%s) || '%%'
        ORDER BY LENGTH(name)
        LIMIT 1
        """,
        parameters=(station_name,)
    )
    if result:
        return result[0]
    
    # 4. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    words = [w for w in station_name.split() if len(w) > 3]
    for word in words:
        result = pg_hook.get_first(
            """
            SELECT id FROM vacancies.metro_stations_by 
            WHERE LOWER(name) LIKE '%%' || LOWER(%s) || '%%'
            ORDER BY LENGTH(name)
            LIMIT 1
            """,
            parameters=(word,)
        )
        if result:
            return result[0]
    
    return None

def extract_coordinates(soup: BeautifulSoup) -> tuple:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
    lat, lng = None, None
    
    # –í data-–∞—Ç—Ä–∏–±—É—Ç–∞—Ö
    map_container = soup.find(attrs={'data-latitude': True, 'data-longitude': True})
    if map_container:
        try:
            lat = float(map_container.get('data-latitude'))
            lng = float(map_container.get('data-longitude'))
        except (ValueError, TypeError):
            pass
    
    # –í JavaScript
    if not lat:
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                patterns = [r'"latitude":\s*([\d.]+)', r'"lat":\s*([\d.]+)']
                for pattern in patterns:
                    lat_match = re.search(pattern, script.string)
                    if lat_match:
                        lng_pattern = pattern.replace('lat', 'lng').replace('latitude', 'longitude')
                        lng_match = re.search(lng_pattern, script.string)
                        if lng_match:
                            try:
                                lat = float(lat_match.group(1))
                                lng = float(lng_match.group(1))
                                break
                            except ValueError:
                                continue
                if lat:
                    break
    
    return lat, lng


def parse_salary(element) -> Optional[Dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—Ä–ø–ª–∞—Ç—ã"""
    if not element:
        return None
    
    text = element.get_text(strip=True) if hasattr(element, 'get_text') else str(element)
    
    numbers = re.findall(r'(\d+(?:\s?\d{3})*)', text)
    numbers = [int(n.replace(' ', '').replace('\xa0', '')) for n in numbers if n]
    
    result = {'from': None, 'to': None, 'currency': 'BYN', 'gross': False}
    
    if numbers:
        if len(numbers) >= 2:
            result['from'] = numbers[0]
            result['to'] = numbers[1]
        elif '–æ—Ç' in text.lower():
            result['from'] = numbers[0]
        elif '–¥–æ' in text.lower():
            result['to'] = numbers[0]
        else:
            result['from'] = numbers[0]
            result['to'] = numbers[0]
    
    if 'USD' in text or '$' in text:
        result['currency'] = 'USD'
    elif 'EUR' in text or '‚Ç¨' in text:
        result['currency'] = 'EUR'
    
    result['gross'] = '–¥–æ –≤—ã—á–µ—Ç–∞' in text.lower() or 'gross' in text.lower()
    result['description'] = text
    
    return result


def parse_vacancy_page(session, vacancy_url):
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –Ω–∞–≤—ã–∫–æ–≤"""
    try:
        response = session.get(vacancy_url, timeout=30)
        response.raise_for_status()
        html_content = response.text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        result = {
            'description_html': '',
            'description_text': '',
            'key_skills': [],
            'salary': None,
            'experience_id': 'noExperience',
            'experience_name': '–ù–µ—Ç –æ–ø—ã—Ç–∞',
            'employment_id': 'full',
            'employment_name': '–ü–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å',
            'schedule_id': 'fullDay',
            'schedule_name': '–ü–æ–ª–Ω—ã–π –¥–µ–Ω—å',
            'address_raw': None,
            'address_city': None,
            'address_street': None,
            'address_building': None,
            'coordinates': {'lat': None, 'lng': None},
            'metro_stations': [],
            'employer_name': None,
            'employer_url': None,
            'employer_trusted': False,
            'premium': False,
            'accept_handicapped': False,
            'accept_kids': False,
            'response_letter_required': False,
            'published_at': None
        }
        
        # === –î–ê–¢–ê –ü–£–ë–õ–ò–ö–ê–¶–ò–ò ===
        published_at = None
        
        # 1. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º JSON-LD
        json_ld = extract_json_ld(soup)
        if json_ld and 'datePosted' in json_ld:
            try:
                published_at = date_parser.parse(json_ld['datePosted'])
                logger.debug(f"Got date from JSON-LD: {published_at}")
            except Exception as e:
                logger.debug(f"Failed to parse JSON-LD date: {e}")
        
        # 2. –ü–∞—Ä—Å–∏–º –∏–∑ HTML
        if not published_at:
            date_selectors = [
                'p[data-qa="vacancy-creation-time-redesigned"]',
                'p[data-qa="vacancy-creation-time"]',
                'p[data-qa="vacancy-view-creation-date"]',
                'span.vacancy-creation-time-redesigned',
                'span.vacancy-creation-time',
                'span[data-qa="vacancy-serp__vacancy-date"]'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    published_at = parse_russian_date(date_text)
                    if published_at:
                        logger.debug(f"Parsed date from HTML: {published_at}")
                        break
        
        # 3. Fallback –Ω–∞ —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É
        if not published_at:
            published_at = datetime.now()
        
        result['published_at'] = published_at
        
        # === –†–ê–ë–û–¢–û–î–ê–¢–ï–õ–¨ ===
        employer_selectors = [
            'a[data-qa="vacancy-company-name"]',
            'span[data-qa="vacancy-company-name"]'
        ]
        
        for selector in employer_selectors:
            employer_elem = soup.select_one(selector)
            if employer_elem:
                result['employer_name'] = employer_elem.get_text(strip=True)
                if employer_elem.name == 'a':
                    result['employer_url'] = urljoin(RABOTA_BASE_URL, employer_elem.get('href', ''))
                break
        
        if soup.find(class_=re.compile(r'vacancy-company-trusted|verified')):
            result['employer_trusted'] = True
        
        # === –û–ü–ò–°–ê–ù–ò–ï ===
        desc_selectors = [
            'div[data-qa="vacancy-description"]',
            'div.vacancy-description',
            'div.g-user-content'
        ]
        
        for selector in desc_selectors:
            desc_elem = soup.select_one(selector)
            if desc_elem:
                result['description_html'] = str(desc_elem)
                result['description_text'] = desc_elem.get_text(separator='\n', strip=True)
                break
        
        # === –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–í–´–ö–ò (–£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï) ===
        result['key_skills'] = extract_key_skills_from_page(html_content)
        
        # –ï—Å–ª–∏ –Ω–∞–≤—ã–∫–æ–≤ –Ω–µ—Ç, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        if not result['key_skills'] and result['description_text']:
            logger.warning(f"No skills found in structured data for {vacancy_url}, trying description fallback")
            result['key_skills'] = extract_skills_from_description(result['description_text'])
            if result['key_skills']:
                logger.info(f"‚úÖ Found {len(result['key_skills'])} technologies in description")
        
        # === –ó–ê–†–ü–õ–ê–¢–ê ===
        salary_elem = soup.select_one('span[data-qa="vacancy-salary"]') or \
                     soup.select_one('div[data-qa="vacancy-salary"]')
        if salary_elem:
            result['salary'] = parse_salary(salary_elem)
        
        # === –û–ü–´–¢ ===
        exp_elem = soup.select_one('span[data-qa="vacancy-experience"]') or \
                  soup.select_one('p[data-qa="vacancy-experience"]')
        if exp_elem:
            exp_text = exp_elem.get_text(strip=True).lower()
            if '–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è' in exp_text or '–±–µ–∑ –æ–ø—ã—Ç–∞' in exp_text:
                result['experience_id'] = 'noExperience'
                result['experience_name'] = '–ù–µ—Ç –æ–ø—ã—Ç–∞'
            elif any(x in exp_text for x in ['1 –≥–æ–¥', '1-3', '–æ—Ç 1', '1‚Äì3']):
                result['experience_id'] = 'between1And3'
                result['experience_name'] = '–û—Ç 1 –≥–æ–¥–∞ –¥–æ 3 –ª–µ—Ç'
            elif any(x in exp_text for x in ['3 –≥–æ–¥–∞', '3-6', '–æ—Ç 3', '3‚Äì6']):
                result['experience_id'] = 'between3And6'
                result['experience_name'] = '–û—Ç 3 –¥–æ 6 –ª–µ—Ç'
            elif any(x in exp_text for x in ['–±–æ–ª–µ–µ 6', '–æ—Ç 6', '6 –ª–µ—Ç']):
                result['experience_id'] = 'moreThan6'
                result['experience_name'] = '–ë–æ–ª–µ–µ 6 –ª–µ—Ç'
        
        # === –¢–ò–ü –ó–ê–ù–Ø–¢–û–°–¢–ò ===
        emp_elem = soup.select_one('p[data-qa="vacancy-employment"]') or \
                  soup.select_one('span[data-qa="vacancy-employment"]')
        if emp_elem:
            emp_text = emp_elem.get_text(strip=True).lower()
            if '—á–∞—Å—Ç–∏—á–Ω–∞—è' in emp_text:
                result['employment_id'] = 'part'
                result['employment_name'] = '–ß–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å'
            elif '–ø—Ä–æ–µ–∫—Ç' in emp_text:
                result['employment_id'] = 'project'
                result['employment_name'] = '–ü—Ä–æ–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞'
            elif '—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞' in emp_text:
                result['employment_id'] = 'probation'
                result['employment_name'] = '–°—Ç–∞–∂–∏—Ä–æ–≤–∫–∞'
        
        # === –ì–†–ê–§–ò–ö ===
        schedule_elem = soup.select_one('p[data-qa="vacancy-schedule"]') or \
                       soup.select_one('span[data-qa="vacancy-schedule"]')
        if schedule_elem:
            schedule_text = schedule_elem.get_text(strip=True).lower()
            if '—É–¥–∞–ª–µ–Ω' in schedule_text or 'remote' in schedule_text:
                result['schedule_id'] = 'remote'
                result['schedule_name'] = '–£–¥–∞–ª—ë–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞'
            elif '–≥–∏–±–∫' in schedule_text:
                result['schedule_id'] = 'flexible'
                result['schedule_name'] = '–ì–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫'
            elif '—Å–º–µ–Ω' in schedule_text:
                result['schedule_id'] = 'shift'
                result['schedule_name'] = '–°–º–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫'
        
        # === –ê–î–†–ï–° ===
        address_elem = soup.select_one('span[data-qa="vacancy-view-raw-address"]') or \
                      soup.select_one('div[data-qa="vacancy-address"]')
        if address_elem:
            address_text = address_elem.get_text(strip=True)
            result['address_raw'] = address_text
            parsed_address = parse_complex_address(address_text)
            result['address_city'] = parsed_address['city']
            result['address_street'] = parsed_address['street']
            result['address_building'] = parsed_address['building']
            result['metro_stations'] = parsed_address['metro_stations']
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if parsed_address['metro_stations']:
                logger.info(f"üöá Parsed metro stations from address '{address_text}': {parsed_address['metro_stations']}")
            else:
                logger.debug(f"üìç No metro stations found in address: '{address_text}'")
        
        # === –ö–û–û–†–î–ò–ù–ê–¢–´ ===
        lat, lng = extract_coordinates(soup)
        if lat and lng:
            result['coordinates']['lat'] = lat
            result['coordinates']['lng'] = lng
        
        return result
        
    except Exception as e:
        logger.error(f"Error parsing vacancy page {vacancy_url}: {str(e)}")
        return None


def fetch_dictionaries(**context):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤"""
    logger.info("Starting dictionaries fetch")
    
    pg_hook = PostgresHook(postgres_conn_id='vacancy_postgres')
    stats = {'areas': 0}
    
    try:
        belarus_regions = [
            {"id": "16", "name": "–ë–µ–ª–∞—Ä—É—Å—å", "parent_id": None},
            {"id": "1002", "name": "–ú–∏–Ω—Å–∫", "parent_id": "16"},
            {"id": "1003", "name": "–ë—Ä–µ—Å—Ç—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "parent_id": "16"},
            {"id": "1004", "name": "–í–∏—Ç–µ–±—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "parent_id": "16"},
            {"id": "1005", "name": "–ì–æ–º–µ–ª—å—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "parent_id": "16"},
            {"id": "1006", "name": "–ì—Ä–æ–¥–Ω–µ–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "parent_id": "16"},
            {"id": "1007", "name": "–ú–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "parent_id": "16"},
            {"id": "1008", "name": "–ú–æ–≥–∏–ª–µ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å", "parent_id": "16"},
        ]
        
        for region in belarus_regions:
            pg_hook.run(
                """
                INSERT INTO vacancies.areas (id, name, parent_id)
                VALUES (%s, %s, %s)
                ON CONFLICT (id) DO UPDATE 
                SET name = EXCLUDED.name, parent_id = EXCLUDED.parent_id
                """,
                parameters=(region['id'], region['name'], region['parent_id'])
            )
            stats['areas'] += 1
        
        logger.info(f"Loaded {stats['areas']} areas")
        
        context['ti'].xcom_push(key='dict_stats', value=stats)
        return stats
        
    except Exception as e:
        logger.error(f"Error in fetch_dictionaries: {str(e)}")
        raise


def fetch_new_vacancies(**context):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å rabota.by —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–∞–≤—ã–∫–æ–≤"""
    logger.info("Starting new vacancies fetch from rabota.by")
    
    pg_hook = PostgresHook(postgres_conn_id='vacancy_postgres')
    session = get_rabota_session()

    # === –ü–†–û–í–ï–†–ö–ê –°–ü–†–ê–í–û–ß–ù–ò–ö–ê –ú–ï–¢–†–û ===
    try:
        metro_count = pg_hook.get_first("SELECT COUNT(*) FROM vacancies.metro_stations_by")
        logger.info(f"üìä Metro stations in metro_stations_by: {metro_count[0] if metro_count else 0}")
        
        # –í—ã–≤–æ–¥–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        sample_stations = pg_hook.get_records("SELECT id, name FROM vacancies.metro_stations_by LIMIT 5")
        if sample_stations:
            logger.info(f"üìä Sample metro stations: {sample_stations}")
    except Exception as e:
        logger.error(f"‚ùå Cannot access metro_stations_by: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å vacancy_metro_by
    try:
        pg_hook.run("SELECT 1 FROM vacancies.vacancy_metro_by LIMIT 1")
        logger.info("‚úÖ Table vacancy_metro_by is accessible")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Table vacancy_metro_by issue: {e}")
        # –ü—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        try:
            pg_hook.run("""
                CREATE TABLE IF NOT EXISTS vacancies.vacancy_metro_by (
                    vacancy_id BIGINT NOT NULL,
                    metro_id INTEGER NOT NULL,
                    PRIMARY KEY (vacancy_id, metro_id)
                )
            """)
            logger.info("‚úÖ Created vacancy_metro_by table")
        except Exception as create_e:
            logger.error(f"‚ùå Cannot create vacancy_metro_by: {create_e}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    search_text = Variable.get('rabota_search_text', default_var='–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç')
    search_area = Variable.get('rabota_search_area', default_var='16')
    max_vacancies = int(Variable.get('rabota_max_vacancies', default_var='200'))
    max_pages = int(Variable.get('rabota_max_pages', default_var='10'))
    
    total_processed = 0
    new_vacancies = 0
    updated_vacancies = 0
    skipped_existing = 0
    failed_saves = 0
    total_skills_saved = 0
    vacancies_with_skills = 0
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID
        existing_vacancies = set()
        result = pg_hook.get_records("SELECT id FROM vacancies.vacancies")
        if result:
            existing_vacancies = {row[0] for row in result}
            logger.info(f"Found {len(existing_vacancies)} existing vacancies in DB")
        
        page = 0
        vacancies_per_page = 20
        consecutive_empty_pages = 0
        
        while page < max_pages and new_vacancies < max_vacancies:
            search_params = {
                'text': search_text,
                'area': search_area,
                'page': page,
                'per_page': vacancies_per_page
            }
            
            logger.info(f"Fetching page {page + 1} of {max_pages}")
            
            response = session.get(RABOTA_SEARCH_URL, params=search_params, timeout=30)
            if response.status_code != 200:
                logger.warning(f"Got status {response.status_code}")
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ "–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
            no_items = soup.find('div', class_='vacancy-search-no-items')
            if no_items:
                logger.info("No more vacancies found - reached end of results")
                break
            
            # –ò—â–µ–º –±–ª–æ–∫–∏ –≤–∞–∫–∞–Ω—Å–∏–π
            vacancy_blocks = []
            
            block_selectors = [
                'div[data-qa="vacancy-serp__vacancy"]',
                'div.vacancy-serp-item',
                'div[class*="vacancy-serp-item"]',
                'article.vacancy-serp-item',
                'div.serp-item'
            ]
            
            for selector in block_selectors:
                blocks = soup.select(selector)
                if blocks:
                    vacancy_blocks = blocks
                    logger.info(f"Found {len(blocks)} vacancy blocks with selector: {selector}")
                    break
            
            if not vacancy_blocks:
                vacancy_links = soup.find_all('a', href=re.compile(r'/vacancy/\d+'))
                if vacancy_links:
                    logger.info(f"Found {len(vacancy_links)} direct vacancy links")
                    vacancy_blocks = [{'link': link} for link in vacancy_links]
                else:
                    consecutive_empty_pages += 1
                    if consecutive_empty_pages >= 2:
                        logger.info("Two consecutive empty pages, stopping")
                        break
                    page += 1
                    continue
            else:
                consecutive_empty_pages = 0
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
            for block in vacancy_blocks:
                if new_vacancies >= max_vacancies:
                    break
                
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
                    if isinstance(block, dict) and 'link' in block:
                        link_elem = block['link']
                    else:
                        link_elem = block.select_one('a[data-qa*="vacancy-title"]') or \
                                   block.select_one('a[href*="/vacancy/"]')
                    
                    if not link_elem:
                        continue
                    
                    href = link_elem.get('href', '')
                    vacancy_id_match = re.search(r'/vacancy/(\d+)', href)
                    
                    if not vacancy_id_match:
                        continue
                    
                    vacancy_id = int(vacancy_id_match.group(1))
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –ª–∏ —É–∂–µ
                    if vacancy_id in existing_vacancies:
                        skipped_existing += 1
                        logger.debug(f"Vacancy {vacancy_id} already exists")
                        continue
                    
                    total_processed += 1
                    
                    # –ü–∞—Ä—Å–∏–º –≤–∞–∫–∞–Ω—Å–∏—é
                    vacancy_url = urljoin(RABOTA_BASE_URL, href)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ rabota.by
                    if 'rabota.by' not in vacancy_url:
                        logger.debug(f"Skipping non-rabota.by URL: {vacancy_url}")
                        continue
                    
                    logger.info(f"Parsing vacancy {vacancy_id}")
                    
                    vacancy_data = parse_vacancy_page(session, vacancy_url)
                    if not vacancy_data:
                        logger.warning(f"Failed to parse vacancy {vacancy_id}")
                        failed_saves += 1
                        continue
                    
                    vacancy_name = link_elem.get_text(strip=True) or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                    
                    # === –†–ê–ë–û–¢–û–î–ê–¢–ï–õ–¨ ===
                    employer_id = None
                    if vacancy_data.get('employer_name'):
                        employer = pg_hook.get_first(
                            "SELECT id FROM vacancies.employers WHERE name = %s",
                            parameters=(vacancy_data['employer_name'][:255],)
                        )
                        
                        if employer:
                            employer_id = employer[0]
                        else:
                            try:
                                result = pg_hook.get_first(
                                    """
                                    INSERT INTO vacancies.employers (name, url, trusted)
                                    VALUES (%s, %s, %s)
                                    RETURNING id
                                    """,
                                    parameters=(
                                        vacancy_data['employer_name'][:255],
                                        (vacancy_data.get('employer_url') or '')[:500],
                                        vacancy_data.get('employer_trusted', False)
                                    )
                                )
                                if result:
                                    employer_id = result[0]
                                    logger.debug(f"Created employer: {vacancy_data['employer_name']} (ID: {employer_id})")
                            except Exception as e:
                                logger.error(f"Failed to create employer: {e}")
                    
                    # === –û–ü–†–ï–î–ï–õ–Ø–ï–ú AREA_ID ===
                    area_id = search_area
                    if vacancy_data.get('address_city'):
                        city_map = {
                            '–ú–∏–Ω—Å–∫': '1002',
                            '–ë—Ä–µ—Å—Ç': '1003', 
                            '–í–∏—Ç–µ–±—Å–∫': '1004',
                            '–ì–æ–º–µ–ª—å': '1005',
                            '–ì—Ä–æ–¥–Ω–æ': '1006',
                            '–ú–æ–≥–∏–ª–µ–≤': '1008'
                        }
                        area_id = city_map.get(vacancy_data['address_city'], search_area)
                    
                    # === –ü–†–û–í–ï–†–Ø–ï–ú –ò –§–û–†–ú–ê–¢–ò–†–£–ï–ú –î–ê–¢–£ ===
                    published_at = vacancy_data.get('published_at')
                    if not isinstance(published_at, datetime):
                        published_at = datetime.now()
                    
                    # === –°–û–•–†–ê–ù–Ø–ï–ú –í–ê–ö–ê–ù–°–ò–Æ ===
                    try:
                        result = pg_hook.run(
                            """
                            INSERT INTO vacancies.vacancies (
                                id, name, url, employer_id, area_id,
                                description_html, description_text,
                                salary_from, salary_to, salary_currency, 
                                salary_gross, salary_description,
                                experience_id, experience_name,
                                employment_id, employment_name,
                                schedule_id, schedule_name,
                                address_raw, address_city, address_street, address_building,
                                address_lat, address_lng,
                                premium, accept_handicapped, accept_kids, response_letter_required,
                                published_at, created_at, archived
                            )
                            VALUES (
                                %s, %s, %s, %s, %s,
                                %s, %s,
                                %s, %s, %s,
                                %s, %s,
                                %s, %s,
                                %s, %s,
                                %s, %s,
                                %s, %s, %s, %s,
                                %s, %s,
                                %s, %s, %s, %s,
                                %s, %s, %s
                            )
                            ON CONFLICT (id) DO UPDATE SET
                                name = EXCLUDED.name,
                                description_text = EXCLUDED.description_text,
                                description_html = EXCLUDED.description_html,
                                salary_from = EXCLUDED.salary_from,
                                salary_to = EXCLUDED.salary_to,
                                published_at = EXCLUDED.published_at,
                                updated_at = CURRENT_TIMESTAMP
                            RETURNING id
                            """,
                            parameters=(
                                vacancy_id,
                                vacancy_name[:500],
                                vacancy_url[:500],
                                employer_id,
                                area_id,
                                (vacancy_data.get('description_html') or '')[:50000],
                                (vacancy_data.get('description_text') or '')[:50000],
                                vacancy_data['salary']['from'] if vacancy_data.get('salary') else None,
                                vacancy_data['salary']['to'] if vacancy_data.get('salary') else None,
                                vacancy_data['salary']['currency'] if vacancy_data.get('salary') else None,
                                vacancy_data['salary']['gross'] if vacancy_data.get('salary') else None,
                                (vacancy_data['salary']['description'] if vacancy_data.get('salary') else '')[:255],
                                vacancy_data.get('experience_id', 'noExperience'),
                                vacancy_data.get('experience_name', '–ù–µ—Ç –æ–ø—ã—Ç–∞')[:100],
                                vacancy_data.get('employment_id', 'full'),
                                vacancy_data.get('employment_name', '–ü–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å')[:100],
                                vacancy_data.get('schedule_id'),
                                (vacancy_data.get('schedule_name') or '')[:100],
                                (vacancy_data.get('address_raw') or '')[:500],
                                (vacancy_data.get('address_city') or '')[:100],
                                (vacancy_data.get('address_street') or '')[:200],
                                (vacancy_data.get('address_building') or '')[:50],
                                vacancy_data['coordinates']['lat'] if vacancy_data.get('coordinates') else None,
                                vacancy_data['coordinates']['lng'] if vacancy_data.get('coordinates') else None,
                                vacancy_data.get('premium', False),
                                vacancy_data.get('accept_handicapped', False),
                                vacancy_data.get('accept_kids', False),
                                vacancy_data.get('response_letter_required', False),
                                published_at,
                                datetime.now(),
                                False
                            )
                        )
                        
                        if result:
                            if vacancy_id in existing_vacancies:
                                updated_vacancies += 1
                                logger.info(f"‚úÖ Updated vacancy {vacancy_id}: {vacancy_name}")
                            else:
                                new_vacancies += 1
                                existing_vacancies.add(vacancy_id)
                                logger.info(f"‚úÖ Added new vacancy {vacancy_id}: {vacancy_name}")
                    
                    except Exception as e:
                        logger.error(f"‚ùå Failed to save vacancy {vacancy_id}: {str(e)}")
                        failed_saves += 1
                        continue
                    
                    # === –°–û–•–†–ê–ù–Ø–ï–ú –ù–ê–í–´–ö–ò (–í–ê–ñ–ù–û!) ===
                    if vacancy_data.get('key_skills'):
                        logger.info(f"üí° Saving {len(vacancy_data['key_skills'])} skills for vacancy {vacancy_id}: {', '.join(vacancy_data['key_skills'][:5])}")
                        
                        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–∞–≤—ã–∫–∏ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
                        pg_hook.run(
                            "DELETE FROM vacancies.vacancy_skills WHERE vacancy_id = %s",
                            parameters=(vacancy_id,)
                        )
                        
                        saved_skills = 0
                        for skill_name in vacancy_data['key_skills']:
                            if skill_name and len(skill_name.strip()) > 1:
                                try:
                                    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–∞–≤—ã–∫ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é –ë–î
                                    skill_id = pg_hook.get_first(
                                        "SELECT vacancies.add_or_get_skill(%s)",
                                        parameters=(skill_name.strip()[:100],)
                                    )
                                    
                                    if skill_id:
                                        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑—å –≤–∞–∫–∞–Ω—Å–∏—è-–Ω–∞–≤—ã–∫
                                        pg_hook.run(
                                            """
                                            INSERT INTO vacancies.vacancy_skills (vacancy_id, skill_id)
                                            VALUES (%s, %s)
                                            ON CONFLICT DO NOTHING
                                            """,
                                            parameters=(vacancy_id, skill_id[0])
                                        )
                                        saved_skills += 1
                                except Exception as e:
                                    logger.error(f"Failed to add skill '{skill_name}': {e}")
                        
                        if saved_skills > 0:
                            total_skills_saved += saved_skills
                            vacancies_with_skills += 1
                            logger.info(f"‚úÖ Saved {saved_skills}/{len(vacancy_data['key_skills'])} skills for vacancy {vacancy_id}")
                        else:
                            logger.warning(f"‚ö†Ô∏è Could not save any skills for vacancy {vacancy_id}")
                    else:
                        logger.warning(f"‚ö†Ô∏è No skills found for vacancy {vacancy_id}: {vacancy_name}")
                    
                    # === –°–û–•–†–ê–ù–Ø–ï–ú –°–¢–ê–ù–¶–ò–ò –ú–ï–¢–†–û ===
                    if vacancy_data.get('metro_stations'):
                        logger.info(f"üöá Processing {len(vacancy_data['metro_stations'])} metro stations for vacancy {vacancy_id}: {vacancy_data['metro_stations']}")
                        
                        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å–≤—è–∑–∏ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
                        try:
                            pg_hook.run(
                                "DELETE FROM vacancies.vacancy_metro_by WHERE vacancy_id = %s",
                                parameters=(vacancy_id,)
                            )
                        except Exception as e:
                            logger.debug(f"Could not delete old metro links: {e}")
                        
                        saved_metro = 0
                        not_found_stations = []
                        
                        for station_name in vacancy_data['metro_stations']:
                            if not station_name or len(station_name.strip()) < 2:
                                continue
                            
                            station_name = station_name.strip()
                            
                            try:
                                # –ò—â–µ–º —Å—Ç–∞–Ω—Ü–∏—é –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ
                                metro_id = find_metro_station_id(pg_hook, station_name)
                                
                                if metro_id:
                                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑—å –≤–∞–∫–∞–Ω—Å–∏—è-–º–µ—Ç—Ä–æ
                                    pg_hook.run(
                                        """
                                        INSERT INTO vacancies.vacancy_metro_by (vacancy_id, metro_id)
                                        VALUES (%s, %s)
                                        ON CONFLICT DO NOTHING
                                        """,
                                        parameters=(vacancy_id, metro_id)
                                    )
                                    saved_metro += 1
                                    logger.debug(f"‚úÖ Linked metro '{station_name}' (ID: {metro_id}) to vacancy {vacancy_id}")
                                else:
                                    not_found_stations.append(station_name)
                                    
                            except Exception as e:
                                logger.error(f"Failed to link metro station '{station_name}': {e}")
                        
                        if saved_metro > 0:
                            logger.info(f"‚úÖ Saved {saved_metro}/{len(vacancy_data['metro_stations'])} metro stations for vacancy {vacancy_id}")
                        
                        if not_found_stations:
                            logger.warning(f"‚ö†Ô∏è Metro stations not found in DB for vacancy {vacancy_id}: {not_found_stations}")
                    else:
                        logger.debug(f"No metro stations in address for vacancy {vacancy_id}")
                    time.sleep(0.3)
                
                except Exception as e:
                    logger.error(f"Unexpected error processing vacancy: {str(e)}")
                    failed_saves += 1
                    continue
            
            page += 1
            logger.info(f"Page {page} completed: {new_vacancies} new, {updated_vacancies} updated")
            time.sleep(0.5)
        
        # === –°–û–•–†–ê–ù–Ø–ï–ú –°–¢–ê–¢–ò–°–¢–ò–ö–£ ===
        try:
            pg_hook.run(
                """
                INSERT INTO vacancies.parse_statistics 
                (source, total_processed, new_vacancies, updated_vacancies, failed_vacancies, error_messages)
                VALUES (%s, %s, %s, %s, %s, %s)
                """,
                parameters=(
                    'rabota.by', 
                    total_processed, 
                    new_vacancies, 
                    updated_vacancies, 
                    failed_saves,
                    f"Skills: {total_skills_saved} total, {vacancies_with_skills} vacancies with skills"
                )
            )
        except Exception as e:
            logger.error(f"Failed to save statistics: {e}")
        
        logger.info(f"""
        ‚úÖ Parsing completed:
        - Total processed: {total_processed}
        - New vacancies: {new_vacancies}
        - Updated vacancies: {updated_vacancies}
        - Skipped existing: {skipped_existing}
        - Failed to save: {failed_saves}
        - Total skills saved: {total_skills_saved}
        - Vacancies with skills: {vacancies_with_skills}
        """)
        
        context['ti'].xcom_push(key='new_vacancies', value=new_vacancies)
        context['ti'].xcom_push(key='updated_vacancies', value=updated_vacancies)
        context['ti'].xcom_push(key='total_processed', value=total_processed)
        context['ti'].xcom_push(key='total_skills_saved', value=total_skills_saved)
        
        return {
            'new': new_vacancies, 
            'updated': updated_vacancies,
            'skipped': skipped_existing, 
            'failed': failed_saves,
            'total': total_processed,
            'skills_saved': total_skills_saved,
            'vacancies_with_skills': vacancies_with_skills
        }
        
    except Exception as e:
        logger.error(f"Critical error in fetch_new_vacancies: {str(e)}")
        raise


def debug_metro_parsing(**context):
    """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–µ—Ç—Ä–æ"""
    
    pg_hook = PostgresHook(postgres_conn_id='vacancy_postgres')
    session = get_rabota_session()
    
    logger.info("=" * 60)
    logger.info("üîç DEBUG: Starting metro parsing diagnostic")
    logger.info("=" * 60)
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –º–µ—Ç—Ä–æ
    logger.info("\nüìä Step 1: Checking metro_stations_by table...")
    try:
        metro_count = pg_hook.get_first("SELECT COUNT(*) FROM vacancies.metro_stations_by")
        logger.info(f"   Total stations: {metro_count[0] if metro_count else 0}")
        
        sample = pg_hook.get_records("SELECT id, name FROM vacancies.metro_stations_by ORDER BY id LIMIT 10")
        logger.info(f"   Sample stations: {sample}")
    except Exception as e:
        logger.error(f"   ‚ùå ERROR accessing metro_stations_by: {e}")
        return
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–±–ª–∏—Ü—É —Å–≤—è–∑–µ–π
    logger.info("\nüìä Step 2: Checking vacancy_metro_by table...")
    try:
        link_count = pg_hook.get_first("SELECT COUNT(*) FROM vacancies.vacancy_metro_by")
        logger.info(f"   Total links: {link_count[0] if link_count else 0}")
    except Exception as e:
        logger.error(f"   ‚ùå ERROR accessing vacancy_metro_by: {e}")
        # –ü—Ä–æ–±—É–µ–º –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        try:
            columns = pg_hook.get_records("""
                SELECT column_name, data_type 
                FROM information_schema.columns 
                WHERE table_name = 'vacancy_metro_by'
            """)
            logger.info(f"   Table columns: {columns}")
        except:
            pass
    
    # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∞–¥—Ä–µ—Å–∞
    logger.info("\nüìä Step 3: Testing parse_complex_address...")
    test_addresses = [
        "–ú–∏–Ω—Å–∫,–ú–æ–ª–æ–¥–µ–∂–Ω–∞—è,–ü–ª–æ—â–∞–¥—å –§—Ä–∞–Ω—Ç–∏—à–∫–∞ –ë–æ–≥—É—à–µ–≤–∏—á–∞,–§—Ä—É–Ω–∑–µ–Ω—Å–∫–∞—è, —É–ª–∏—Ü–∞ –¢–∏–º–∏—Ä—è–∑–µ–≤–∞, 9–∫10",
        "–ú–∏–Ω—Å–∫, –ù–µ–º–∏–≥–∞, —É–ª. –ù–µ–º–∏–≥–∞, 5",
        "–ú–∏–Ω—Å–∫, –º. –ü–ª–æ—â–∞–¥—å –õ–µ–Ω–∏–Ω–∞, –ø—Ä. –ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, 10",
        "–ú–∏–Ω—Å–∫, –û–∫—Ç—è–±—Ä—å—Å–∫–∞—è, –ü–ª–æ—â–∞–¥—å –ü–æ–±–µ–¥—ã, —É–ª–∏—Ü–∞ –ö–∞—Ä–ª–∞ –ú–∞—Ä–∫—Å–∞, 25"
    ]
    
    for addr in test_addresses:
        parsed = parse_complex_address(addr)
        logger.info(f"\n   Address: {addr}")
        logger.info(f"   Parsed city: {parsed['city']}")
        logger.info(f"   Parsed street: {parsed['street']}")
        logger.info(f"   Parsed building: {parsed['building']}")
        logger.info(f"   Parsed metro: {parsed['metro_stations']}")
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Ç–∞–Ω—Ü–∏–∏ –≤ –ë–î
        for station in parsed['metro_stations']:
            metro_id = find_metro_station_id(pg_hook, station)
            if metro_id:
                logger.info(f"   ‚úÖ Station '{station}' -> ID: {metro_id}")
            else:
                logger.warning(f"   ‚ùå Station '{station}' -> NOT FOUND")
    
    # 4. –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–Ω—É —Ä–µ–∞–ª—å–Ω—É—é –≤–∞–∫–∞–Ω—Å–∏—é
    logger.info("\nüìä Step 4: Testing real vacancy parsing...")
    try:
        # –ü–æ–ª—É—á–∞–µ–º –æ–¥–Ω—É –≤–∞–∫–∞–Ω—Å–∏—é —Å –∞–¥—Ä–µ—Å–æ–º
        response = session.get(
            RABOTA_SEARCH_URL, 
            params={'text': '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', 'area': '16', 'page': 0},
            timeout=30
        )
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –≤–∞–∫–∞–Ω—Å–∏—é
        link_elem = soup.select_one('a[data-qa*="vacancy-title"]') or \
                    soup.select_one('a[href*="/vacancy/"]')
        
        if link_elem:
            href = link_elem.get('href', '')
            vacancy_url = urljoin(RABOTA_BASE_URL, href)
            logger.info(f"   Testing vacancy: {vacancy_url}")
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            vacancy_data = parse_vacancy_page(session, vacancy_url)
            
            if vacancy_data:
                logger.info(f"   Address raw: {vacancy_data.get('address_raw')}")
                logger.info(f"   Address city: {vacancy_data.get('address_city')}")
                logger.info(f"   Metro stations: {vacancy_data.get('metro_stations')}")
                
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–∞–∂–¥—É—é —Å—Ç–∞–Ω—Ü–∏—é
                for station in vacancy_data.get('metro_stations', []):
                    metro_id = find_metro_station_id(pg_hook, station)
                    logger.info(f"   Station '{station}' -> DB ID: {metro_id}")
            else:
                logger.error("   ‚ùå Failed to parse vacancy page")
        else:
            logger.error("   ‚ùå No vacancy links found")
            
    except Exception as e:
        logger.error(f"   ‚ùå Error testing real vacancy: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    # 5. –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤—Å—Ç–∞–≤–∫—É
    logger.info("\nüìä Step 5: Testing INSERT into vacancy_metro_by...")
    try:
        # –ë–µ—Ä—ë–º –ª—é–±—É—é —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤–∞–∫–∞–Ω—Å–∏—é
        vacancy = pg_hook.get_first("SELECT id FROM vacancies.vacancies LIMIT 1")
        if vacancy:
            vacancy_id = vacancy[0]
            # –ë–µ—Ä—ë–º –ª—é–±—É—é —Å—Ç–∞–Ω—Ü–∏—é –º–µ—Ç—Ä–æ
            metro = pg_hook.get_first("SELECT id FROM vacancies.metro_stations_by LIMIT 1")
            if metro:
                metro_id = metro[0]
                
                logger.info(f"   Testing INSERT: vacancy_id={vacancy_id}, metro_id={metro_id}")
                
                # –ü—Ä–æ–±—É–µ–º –≤—Å—Ç–∞–≤–∏—Ç—å
                pg_hook.run(
                    """
                    INSERT INTO vacancies.vacancy_metro_by (vacancy_id, metro_id)
                    VALUES (%s, %s)
                    ON CONFLICT DO NOTHING
                    """,
                    parameters=(vacancy_id, metro_id)
                )
                logger.info("   ‚úÖ INSERT succeeded!")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º
                check = pg_hook.get_first(
                    "SELECT * FROM vacancies.vacancy_metro_by WHERE vacancy_id = %s AND metro_id = %s",
                    parameters=(vacancy_id, metro_id)
                )
                logger.info(f"   Verification: {check}")
                
                # –£–¥–∞–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –∑–∞–ø–∏—Å—å
                pg_hook.run(
                    "DELETE FROM vacancies.vacancy_metro_by WHERE vacancy_id = %s AND metro_id = %s",
                    parameters=(vacancy_id, metro_id)
                )
                logger.info("   Cleaned up test record")
    except Exception as e:
        logger.error(f"   ‚ùå INSERT test failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
    
    logger.info("\n" + "=" * 60)
    logger.info("üîç DEBUG: Diagnostic complete")
    logger.info("=" * 60)

# –°–æ–∑–¥–∞–Ω–∏–µ DAG
dag = DAG(
    'rabota_fetch_new_vacancies',
    default_args=default_args,
    description='–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π —Å rabota.by —Å –ø–æ–ª–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –Ω–∞–≤—ã–∫–æ–≤',
    schedule_interval=timedelta(hours=6),
    catchup=False,
    tags=['rabota', 'vacancies', 'etl', 'skills']
)

# –ó–∞–¥–∞—á–∏
fetch_dictionaries_task = PythonOperator(
    task_id='fetch_dictionaries',
    python_callable=fetch_dictionaries,
    dag=dag
)

fetch_new_vacancies_task = PythonOperator(
    task_id='fetch_new_vacancies',
    python_callable=fetch_new_vacancies,
    dag=dag
)

# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
fetch_dictionaries_task  >> fetch_new_vacancies_task