"""
Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ñ rabota.by (Ğ±ĞµĞ· Selenium)
Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸ÑÑ… Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²ÑĞµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²,
Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğµ
"""

import json
import re
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict, field
from urllib.parse import urljoin, quote

import requests
from bs4 import BeautifulSoup

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹
RABOTA_BASE_URL = "https://rabota.by"
SEARCH_URL = f"{RABOTA_BASE_URL}/search/vacancy"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
MAX_VACANCIES = 10  # ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°

@dataclass
class Employer:
    """Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»Ğµ"""
    id: Optional[int] = None
    name: Optional[str] = None
    url: Optional[str] = None
    logo_url: Optional[str] = None
    trusted: bool = False
    description: Optional[str] = None
    website: Optional[str] = None
    
@dataclass
class Address:
    """ĞĞ´Ñ€ĞµÑ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸"""
    city: Optional[str] = None
    street: Optional[str] = None
    building: Optional[str] = None
    lat: Optional[float] = None
    lng: Optional[float] = None
    metro_stations: List[str] = field(default_factory=list)
    raw: Optional[str] = None

@dataclass 
class Salary:
    """Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğµ"""
    from_value: Optional[int] = None
    to_value: Optional[int] = None
    currency: Optional[str] = None
    gross: Optional[bool] = None
    description: Optional[str] = None

@dataclass
class Vacancy:
    """ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸"""
    id: int
    name: str
    url: str
    employer: Employer
    published_at: Optional[str] = None
    created_at: Optional[str] = None
    archived: bool = False
    premium: bool = False
    
    # ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ (ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ)
    description_html: Optional[str] = None
    description_text: Optional[str] = None
    
    # ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº Ğ½Ğ° ÑĞ°Ğ¹Ñ‚Ğµ)
    key_skills: List[str] = field(default_factory=list)
    
    # Ğ£ÑĞ»Ğ¾Ğ²Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
    salary: Optional[Salary] = None
    experience_id: Optional[str] = None
    experience_name: Optional[str] = None
    schedule_id: Optional[str] = None  
    schedule_name: Optional[str] = None
    employment_id: Optional[str] = None
    employment_name: Optional[str] = None
    
    # Ğ”Ğ»Ñ ĞºĞ°Ñ€Ñ‚Ñ‹
    address: Optional[Address] = None
    working_days: List[str] = field(default_factory=list)
    working_time_intervals: List[str] = field(default_factory=list)
    working_time_modes: List[str] = field(default_factory=list)
    
    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
    accept_handicapped: bool = False
    accept_kids: bool = False
    specializations: List[Dict[str, str]] = field(default_factory=list)
    professional_roles: List[Dict[str, str]] = field(default_factory=list)
    languages: List[Dict[str, str]] = field(default_factory=list)
    driver_license_types: List[str] = field(default_factory=list)
    
    # ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹
    contacts: Optional[Dict[str, Any]] = None
    response_letter_required: bool = False
    response_url: Optional[str] = None
    test: Optional[Dict[str, str]] = None
    
    # ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    alternate_url: Optional[str] = None
    apply_alternate_url: Optional[str] = None
    code: Optional[str] = None
    department: Optional[Dict[str, str]] = None
    area: Optional[Dict[str, str]] = None

class RabotaByParser:
    """ĞŸĞ°Ñ€ÑĞµÑ€ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ñ ÑĞ°Ğ¹Ñ‚Ğ° rabota.by"""
    
    def __init__(self):
        self.session = self._create_session()
            
    def _create_session(self) -> requests.Session:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ HTTP ÑĞµÑÑĞ¸Ğ¸ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼Ğ¸"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,be;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://rabota.by/',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        })
        return session
    
    def _extract_json_ld(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… JSON-LD"""
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, list):
                    for item in data:
                        if item.get('@type') == 'JobPosting':
                            return item
                elif data.get('@type') == 'JobPosting':
                    return data
            except (json.JSONDecodeError, AttributeError):
                continue
        return None
    
    def _extract_initial_state(self, html: str) -> Optional[Dict]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ __INITIAL_STATE__ Ğ¸Ğ· HTML"""
        match = re.search(r'window\.__INITIAL_STATE__\s*=\s*({.+?});', html, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass
        return None
    
    def _extract_vacancy_data(self, html: str) -> Optional[Dict]:
        """
        Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… JSON ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ
        """
        # Ğ˜Ñ‰ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ JSON Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        patterns = [
            r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
            r'HH\.VacancyResponsePage\.init\(({.+?})\);',
            r'HH\.globalVacancyData\s*=\s*({.+?});',
            r'"vacancy":\s*({.+?}),\s*"',
            r'data-vacancy-json="([^"]+)"',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html, re.DOTALL)
            if match:
                try:
                    # Ğ•ÑĞ»Ğ¸ ÑÑ‚Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ HTML, Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ
                    if 'data-vacancy-json' in pattern:
                        import html as html_lib
                        json_str = html_lib.unescape(match.group(1))
                    else:
                        json_str = match.group(1)
                    
                    data = json.loads(json_str)
                    return data
                except (json.JSONDecodeError, AttributeError) as e:
                    logger.debug(f"Failed to parse JSON with pattern {pattern}: {e}")
                    continue
        
        return None
    
    def _parse_experience(self, text: str) -> tuple:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹"""
        text_lower = text.lower() if text else ''
        
        if 'Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ' in text_lower or 'Ğ±ĞµĞ· Ğ¾Ğ¿Ñ‹Ñ‚Ğ°' in text_lower or 'Ğ½ĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°' in text_lower:
            return 'noExperience', 'ĞĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°'
        elif any(x in text_lower for x in ['1 Ğ³Ğ¾Ğ´', '1-3', 'Ğ¾Ñ‚ 1', 'Ğ¾Ñ‚ Ğ³Ğ¾Ğ´Ğ°', '1â€“3']):
            return 'between1And3', 'ĞÑ‚ 1 Ğ³Ğ¾Ğ´Ğ° Ğ´Ğ¾ 3 Ğ»ĞµÑ‚'
        elif any(x in text_lower for x in ['3 Ğ³Ğ¾Ğ´Ğ°', '3-6', 'Ğ¾Ñ‚ 3', '3â€“6']):
            return 'between3And6', 'ĞÑ‚ 3 Ğ´Ğ¾ 6 Ğ»ĞµÑ‚'
        elif any(x in text_lower for x in ['Ğ±Ğ¾Ğ»ĞµĞµ 6', 'Ğ¾Ñ‚ 6', '6 Ğ»ĞµÑ‚', 'Ğ±Ğ¾Ğ»ÑŒÑˆĞµ 6']):
            return 'moreThan6', 'Ğ‘Ğ¾Ğ»ĞµĞµ 6 Ğ»ĞµÑ‚'
        
        return None, text
    
    def _parse_employment(self, text: str) -> tuple:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸"""
        text_lower = text.lower() if text else ''
        
        if 'Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ' in text_lower or 'full' in text_lower:
            return 'full', 'ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ'
        elif 'Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ' in text_lower or 'part' in text_lower:
            return 'part', 'Ğ§Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ'
        elif 'Ğ¿Ñ€Ğ¾ĞµĞºÑ‚' in text_lower:
            return 'project', 'ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°'
        elif 'ÑÑ‚Ğ°Ğ¶Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°' in text_lower or 'intern' in text_lower:
            return 'probation', 'Ğ¡Ñ‚Ğ°Ğ¶Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°'
        
        return None, text
    
    def _parse_schedule(self, text: str) -> tuple:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹"""
        text_lower = text.lower() if text else ''
        
        if 'ÑƒĞ´Ğ°Ğ»ĞµĞ½' in text_lower or 'remote' in text_lower:
            return 'remote', 'Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°'
        elif 'Ğ³Ğ¸Ğ±Ğº' in text_lower or 'flexible' in text_lower:
            return 'flexible', 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº'
        elif 'ÑĞ¼ĞµĞ½' in text_lower:
            return 'shift', 'Ğ¡Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº'
        elif 'Ğ²Ğ°Ñ…Ñ‚' in text_lower:
            return 'flyInFlyOut', 'Ğ’Ğ°Ñ…Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´'
        elif '5/2' in text_lower or 'Ğ¿ÑÑ‚Ğ¸Ğ´Ğ½ĞµĞ²ĞºĞ°' in text_lower:
            return 'fullDay', 'ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ'
        
        return None, text
    
    def _parse_salary(self, element) -> Optional[Salary]:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğµ"""
        if not element:
            return None
            
        salary = Salary()
        text = element.get_text(strip=True) if hasattr(element, 'get_text') else str(element)
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞµĞ»
        numbers = re.findall(r'(\d+(?:\s?\d{3})*)', text)
        numbers = [int(n.replace(' ', '').replace('\xa0', '')) for n in numbers if n]
        
        if numbers:
            if len(numbers) >= 2:
                salary.from_value = numbers[0]
                salary.to_value = numbers[1]
            elif 'Ğ¾Ñ‚' in text.lower():
                salary.from_value = numbers[0]
            elif 'Ğ´Ğ¾' in text.lower():
                salary.to_value = numbers[0]
            else:
                salary.from_value = numbers[0]
                salary.to_value = numbers[0]
        
        # Ğ’Ğ°Ğ»ÑÑ‚Ğ°
        if 'USD' in text or '$' in text:
            salary.currency = 'USD'
        elif 'EUR' in text or 'â‚¬' in text:
            salary.currency = 'EUR'
        elif 'BYN' in text or 'Ñ€ÑƒĞ±' in text.lower() or 'br' in text.lower():
            salary.currency = 'BYN'
        else:
            salary.currency = 'BYN'
        
        # Ğ“Ñ€Ğ¾ÑÑ/Ğ½ĞµÑ‚
        salary.gross = 'Ğ´Ğ¾ Ğ²Ñ‹Ñ‡ĞµÑ‚Ğ°' in text.lower() or 'gross' in text.lower()
        salary.description = text
        
        return salary
    
    def _parse_complex_address(self, address_text: str) -> Dict[str, Any]:
        """
        Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ñ€ĞµÑĞ°
        ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: 'ĞœĞ¸Ğ½ÑĞº,ĞœĞ¾Ğ»Ğ¾Ğ´ĞµĞ¶Ğ½Ğ°Ñ,ĞŸĞ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¤Ñ€Ğ°Ğ½Ñ‚Ğ¸ÑˆĞºĞ° Ğ‘Ğ¾Ğ³ÑƒÑˆĞµĞ²Ğ¸Ñ‡Ğ°,Ğ¤Ñ€ÑƒĞ½Ğ·ĞµĞ½ÑĞºĞ°Ñ,Ğ®Ğ±Ğ¸Ğ»ĞµĞ¹Ğ½Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ, ÑƒĞ»Ğ¸Ñ†Ğ° Ğ¢Ğ¸Ğ¼Ğ¸Ñ€ÑĞ·ĞµĞ²Ğ°, 9Ğº10'
        Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°: Ğ³Ğ¾Ñ€Ğ¾Ğ´ ÑĞ»ĞµĞ²Ğ°, Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ»Ğ¸Ñ†Ğ° ÑĞ¿Ñ€Ğ°Ğ²Ğ°, Ğ²ÑÑ‘ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸ - Ğ¼ĞµÑ‚Ñ€Ğ¾
        """
        result = {
            'city': None,
            'street': None,
            'building': None,
            'metro_stations': []
        }
        
        if not address_text:
            return result
        
        # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ°Ğ´Ñ€ĞµÑ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸
        parts = [p.strip() for p in re.split(r'[,;]', address_text) if p.strip()]
        
        if not parts:
            return result
        
        # 1. ĞŸĞµÑ€Ğ²Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ - Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ³Ğ¾Ñ€Ğ¾Ğ´
        city_keywords = ['ĞœĞ¸Ğ½ÑĞº', 'Ğ‘Ñ€ĞµÑÑ‚', 'Ğ’Ğ¸Ñ‚ĞµĞ±ÑĞº', 'Ğ“Ğ¾Ğ¼ĞµĞ»ÑŒ', 'Ğ“Ñ€Ğ¾Ğ´Ğ½Ğ¾', 'ĞœĞ¾Ğ³Ğ¸Ğ»ĞµĞ²', 'Ğ‘Ğ¾Ğ±Ñ€ÑƒĞ¹ÑĞº', 'Ğ‘Ğ°Ñ€Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‡Ğ¸', 'ĞŸĞ¸Ğ½ÑĞº']
        first_part = parts[0]
        if any(keyword in first_part for keyword in city_keywords):
            result['city'] = first_part
            parts = parts[1:]  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ°
        
        if not parts:
            return result
        
        # 2. Ğ˜Ğ´Ñ‘Ğ¼ Ñ ĞºĞ¾Ğ½Ñ†Ğ° Ğ¸ Ğ¸Ñ‰ĞµĞ¼ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ»Ğ¸Ñ†Ñƒ
        # ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ - Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ° Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ´Ğ¾Ğ¼Ğ°
        last_part = parts[-1] if parts else None
        if last_part:
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ»Ğ¸ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ´Ğ¾Ğ¼Ğ°/ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°
            if re.search(r'\d+[Ğ°-Ña-z]?\d*|Ğº\d+|ĞºĞ¾Ñ€Ğ¿ÑƒÑ\s*\d+|ÑÑ‚Ñ€\s*\d+|Ğ´\.\s*\d+', last_part, re.IGNORECASE):
                result['building'] = last_part
                parts = parts[:-1]  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ
        
        # ĞŸÑ€ĞµĞ´Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ (Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ) - Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ° ÑƒĞ»Ğ¸Ñ†Ñƒ
        if parts:
            last_part = parts[-1]
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ ÑÑ‚Ğ¾ ÑƒĞ»Ğ¸Ñ†ĞµĞ¹
            street_keywords = ['ÑƒĞ»Ğ¸Ñ†Ğ°', 'ÑƒĞ».', 'Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚', 'Ğ¿Ñ€.', 'Ğ¿Ñ€-Ñ‚', 'Ğ¿ĞµÑ€ĞµÑƒĞ»Ğ¾Ğº', 'Ğ¿ĞµÑ€.', 
                             'Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ', 'Ğ¿Ğ».', 'Ğ±ÑƒĞ»ÑŒĞ²Ğ°Ñ€', 'Ğ±-Ñ€', 'Ğ¿Ñ€Ğ¾ĞµĞ·Ğ´', 'ÑˆĞ¾ÑÑĞµ', 
                             'Ğ½Ğ°Ğ±ĞµÑ€ĞµĞ¶Ğ½Ğ°Ñ', 'Ğ½Ğ°Ğ±.', 'Ñ‚Ñ€Ğ°ĞºÑ‚']
            if any(keyword in last_part.lower() for keyword in street_keywords):
                result['street'] = last_part
                parts = parts[:-1]  # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑƒĞ»Ğ¸Ñ†Ñƒ
        
        # 3. Ğ’ÑÑ‘ Ñ‡Ñ‚Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ»Ğ¾ÑÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ ÑƒĞ»Ğ¸Ñ†ĞµĞ¹/Ğ´Ğ¾Ğ¼Ğ¾Ğ¼ - ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¾
        for part in parts:
            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¾Ñ‚ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¾
            clean_station = re.sub(r'(?:^|\s)(?:Ğ¼\.|Ğ¼ĞµÑ‚Ñ€Ğ¾|ÑÑ‚\.Ğ¼\.|ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¾)\s*', '', part, flags=re.IGNORECASE)
            clean_station = clean_station.strip()
            
            if clean_station:
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¾
                result['metro_stations'].append(clean_station)
        
        return result
    
    def _extract_coordinates(self, soup: BeautifulSoup) -> tuple:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ¸Ğ· ĞºĞ°Ñ€Ñ‚Ñ‹"""
        lat, lng = None, None
        
        # 1. Ğ’ data-Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ…
        map_container = soup.find(attrs={'data-latitude': True, 'data-longitude': True})
        if map_container:
            try:
                lat = float(map_container.get('data-latitude'))
                lng = float(map_container.get('data-longitude'))
            except (ValueError, TypeError):
                pass
        
        # 2. Ğ’ JavaScript ĞºĞ¾Ğ´Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        if not lat:
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    # Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚
                    patterns = [
                        r'"latitude":\s*([\d.]+)',
                        r'"lat":\s*([\d.]+)',
                        r'latitude["\']?\s*[:=]\s*([\d.]+)',
                        r'lat["\']?\s*[:=]\s*([\d.]+)',
                    ]
                    for pattern in patterns:
                        lat_match = re.search(pattern, script.string)
                        if lat_match:
                            lng_pattern = pattern.replace('lat', 'lng').replace('latitude', 'longitude')
                            lng_match = re.search(lng_pattern, script.string)
                            if lng_match:
                                try:
                                    lat = float(lat_match.group(1))
                                    lng = float(lng_match.group(1))
                                    break
                                except ValueError:
                                    continue
                    if lat:
                        break
        
        # 3. Ğ’ iframe Ñ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¹
        if not lat:
            iframe = soup.find('iframe', src=re.compile(r'maps|yandex'))
            if iframe:
                src = iframe.get('src', '')
                # Yandex Maps
                coord_match = re.search(r'll=([\d.]+)%2C([\d.]+)', src)
                if not coord_match:
                    coord_match = re.search(r'll=([\d.]+),([\d.]+)', src)
                # Google Maps
                if not coord_match:
                    coord_match = re.search(r'!3d([\d.]+)!4d([\d.]+)', src)
                
                if coord_match:
                    try:
                        lat = float(coord_match.group(1))
                        lng = float(coord_match.group(2))
                    except ValueError:
                        pass
        
        # 4. Ğ’ ÑÑÑ‹Ğ»ĞºĞµ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ñƒ
        if not lat:
            map_link = soup.find('a', href=re.compile(r'maps|yandex\.by/maps'))
            if map_link:
                href = map_link.get('href', '')
                coord_match = re.search(r'll=([\d.]+),([\d.]+)', href)
                if not coord_match:
                    coord_match = re.search(r'@([\d.]+),([\d.]+)', href)
                
                if coord_match:
                    try:
                        lat = float(coord_match.group(1))
                        lng = float(coord_match.group(2))
                    except ValueError:
                        pass
        
        return lat, lng
    
    def _extract_key_skills_from_page_data(self, html: str) -> List[str]:
        """
        Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        Ğ˜Ñ‰ĞµĞ¼ Ğ² JSON ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğµ
        """
        key_skills = []
        
        # Ğ˜Ñ‰ĞµĞ¼ keySkills Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… JSON ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ñ…
        patterns = [
            r'"keySkills":\s*{[^}]*"keySkill":\s*\[([^\]]+)\]',
            r'"keySkill":\s*\[([^\]]+)\]',
            r'"key_skills":\s*\[([^\]]+)\]',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html, re.DOTALL)
            if match:
                try:
                    # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ÑÑ‚Ñ€Ğ¾ĞºÑƒ ÑĞ¾ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ²
                    skills_str = match.group(1)
                    # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¸Ğ· ÑÑ‚Ñ€Ğ¾ĞºĞ¸
                    skills = re.findall(r'"([^"]+)"', skills_str)
                    key_skills.extend(skills)
                    if key_skills:
                        logger.info(f"Found {len(key_skills)} key skills from page data")
                        return key_skills
                except Exception as e:
                    logger.debug(f"Error extracting skills from pattern {pattern}: {e}")
        
        # ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ - Ğ¸Ñ‰ĞµĞ¼ Ğ²ĞµÑÑŒ JSON Ğ¾Ğ±ÑŠĞµĞºÑ‚
        vacancy_data = self._extract_vacancy_data(html)
        if vacancy_data:
            # Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ² JSON
            paths = [
                ['keySkills', 'keySkill'],
                ['key_skills'],
                ['skills'],
                ['vacancy', 'keySkills', 'keySkill'],
                ['vacancy', 'key_skills'],
            ]
            
            for path in paths:
                try:
                    current = vacancy_data
                    for key in path:
                        if isinstance(current, dict) and key in current:
                            current = current[key]
                        else:
                            break
                    else:
                        # Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¸ Ğ²ĞµÑÑŒ Ğ¿ÑƒÑ‚ÑŒ
                        if isinstance(current, list):
                            key_skills = [str(s) for s in current]
                            if key_skills:
                                logger.info(f"Found {len(key_skills)} key skills from JSON path {' > '.join(path)}")
                                return key_skills
                except Exception as e:
                    logger.debug(f"Error extracting skills from path {path}: {e}")
        
        return key_skills
    
    def parse_vacancy_list(self, search_text: str = "Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚", 
                          area: str = "16", page: int = 0) -> List[Dict[str, Any]]:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ ÑĞ¿Ğ¸ÑĞºĞ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹"""
        vacancies = []
        
        try:
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
            params = {
                'text': search_text,
                'area': area,  # 16 - Ğ‘ĞµĞ»Ğ°Ñ€ÑƒÑÑŒ
                'page': page
            }
            
            logger.info(f"Fetching search results: {SEARCH_URL}")
            logger.info(f"Parameters: {params}")
            
            response = self.session.get(SEARCH_URL, params=params, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· __INITIAL_STATE__
            initial_state = self._extract_initial_state(response.text)
            if initial_state:
                logger.info("Found __INITIAL_STATE__ data")
                if 'vacancies' in initial_state and 'items' in initial_state['vacancies']:
                    vacancy_items = initial_state['vacancies']['items']
                    logger.info(f"Found {len(vacancy_items)} vacancies in initial state")
                    
                    for item in vacancy_items[:MAX_VACANCIES]:
                        try:
                            vacancy_url = f"{RABOTA_BASE_URL}/vacancy/{item['id']}"
                            logger.info(f"Parsing vacancy from initial state: {vacancy_url}")
                            detailed_info = self.parse_vacancy_page(vacancy_url, initial_data=item)
                            if detailed_info:
                                vacancies.append(detailed_info)
                            time.sleep(0.5)
                        except Exception as e:
                            logger.error(f"Error processing initial state vacancy: {e}")
                            continue
            
            # Ğ•ÑĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ² initial state, Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼ HTML
            if not vacancies:
                logger.info("Parsing HTML for vacancy blocks")
                
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ñ‹
                selectors = [
                    'div[data-qa="vacancy-serp__vacancy"]',
                    'div.vacancy-serp-item',
                    'div[class*="vacancy-serp-item"]',
                    'div.serp-item',
                    'article[data-qa="vacancy-serp__vacancy"]'
                ]
                
                vacancy_blocks = []
                for selector in selectors:
                    blocks = soup.select(selector)
                    if blocks:
                        vacancy_blocks = blocks
                        logger.info(f"Found {len(blocks)} vacancy blocks using selector: {selector}")
                        break
                
                if not vacancy_blocks:
                    # ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ - Ğ¸Ñ‰ĞµĞ¼ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼
                    logger.info("Using alternative approach - searching for vacancy links")
                    vacancy_links = soup.find_all('a', href=re.compile(r'/vacancy/\d+'))
                    logger.info(f"Found {len(vacancy_links)} vacancy links")
                    
                    processed_ids = set()
                    for link in vacancy_links[:MAX_VACANCIES]:
                        href = link.get('href', '')
                        vacancy_id_match = re.search(r'/vacancy/(\d+)', href)
                        if vacancy_id_match:
                            vacancy_id = vacancy_id_match.group(1)
                            if vacancy_id not in processed_ids:
                                processed_ids.add(vacancy_id)
                                vacancy_url = urljoin(RABOTA_BASE_URL, href)
                                logger.info(f"Parsing vacancy: {vacancy_url}")
                                vacancy_data = self.parse_vacancy_page(vacancy_url)
                                if vacancy_data:
                                    vacancies.append(vacancy_data)
                                time.sleep(0.5)
                else:
                    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸
                    for block in vacancy_blocks[:MAX_VACANCIES]:
                        vacancy_info = self._extract_vacancy_info_from_block(block)
                        if vacancy_info and vacancy_info.get('url'):
                            logger.info(f"Parsing vacancy: {vacancy_info['url']}")
                            detailed_info = self.parse_vacancy_page(vacancy_info['url'])
                            if detailed_info:
                                vacancies.append(detailed_info)
                            time.sleep(0.5)
                    
        except Exception as e:
            logger.error(f"Error in parse_vacancy_list: {e}")
            import traceback
            traceback.print_exc()
        
        return vacancies
    
    def _extract_vacancy_info_from_block(self, block) -> Dict[str, Any]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ¸Ğ· Ğ±Ğ»Ğ¾ĞºĞ° ÑĞ¿Ğ¸ÑĞºĞ°"""
        info = {}
        
        try:
            # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑÑ‹Ğ»ĞºĞ°
            title_selectors = [
                'a[data-qa="vacancy-serp__vacancy-title"]',
                'a[data-qa="serp-item__title"]',
                'a.bloko-link',
                'a[href*="/vacancy/"]'
            ]
            
            title_elem = None
            for selector in title_selectors:
                title_elem = block.select_one(selector)
                if title_elem and title_elem.get('href'):
                    break
            
            if title_elem:
                info['name'] = title_elem.get_text(strip=True)
                href = title_elem.get('href', '')
                info['url'] = urljoin(RABOTA_BASE_URL, href)
                
                # ID Ğ¸Ğ· URL
                id_match = re.search(r'/vacancy/(\d+)', href)
                if id_match:
                    info['id'] = int(id_match.group(1))
                
        except Exception as e:
            logger.debug(f"Error extracting vacancy info from block: {e}")
        
        return info
    
    def parse_vacancy_page(self, url: str, initial_data: Dict = None) -> Optional[Dict[str, Any]]:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸"""
        try:
            logger.info(f"Fetching vacancy page: {url}")
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ID Ğ¸Ğ· URL
            id_match = re.search(r'/vacancy/(\d+)', url)
            if not id_match:
                logger.error(f"Cannot extract ID from URL: {url}")
                return None
            
            vacancy_id = int(id_match.group(1))
            
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            html_content = response.text
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ JSON-LD Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            json_ld = self._extract_json_ld(soup)
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹
            employer = Employer()
            address = Address()
            salary = None
            
            # === ĞĞĞ—Ğ’ĞĞĞ˜Ğ• Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ˜ ===
            name = None
            title_selectors = [
                'h1[data-qa="vacancy-title"]',
                'h1.bloko-header-section-1',
                'h1[class*="vacancy-title"]'
            ]
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    name = title_elem.get_text(strip=True)
                    break
            
            if not name and json_ld:
                name = json_ld.get('title')
            
            if not name:
                name = "Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ"
            
            # === Ğ ĞĞ‘ĞĞ¢ĞĞ”ĞĞ¢Ğ•Ğ›Ğ¬ ===
            employer_selectors = [
                'a[data-qa="vacancy-company-name"]',
                'span[data-qa="vacancy-company-name"]',
                'div.vacancy-company-name a',
                'span.vacancy-company-name'
            ]
            
            for selector in employer_selectors:
                employer_elem = soup.select_one(selector)
                if employer_elem:
                    employer.name = employer_elem.get_text(strip=True)
                    if employer_elem.name == 'a':
                        employer.url = urljoin(RABOTA_BASE_URL, employer_elem.get('href', ''))
                    break
            
            if not employer.name and json_ld and 'hiringOrganization' in json_ld:
                employer.name = json_ld['hiringOrganization'].get('name')
            
            # Ğ›Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸
            logo_elem = soup.find('img', class_=re.compile(r'vacancy-company-logo'))
            if logo_elem:
                employer.logo_url = logo_elem.get('src')
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ
            if soup.find(class_=re.compile(r'vacancy-company-trusted|verified')):
                employer.trusted = True
            
            # === ĞĞŸĞ˜Ğ¡ĞĞĞ˜Ğ• Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ˜ (Ğ‘Ğ•Ğ— Ğ ĞĞ—Ğ‘Ğ˜Ğ’ĞšĞ˜ ĞĞ Ğ¡Ğ•ĞšĞ¦Ğ˜Ğ˜) ===
            description_html = ""
            description_text = ""
            
            desc_selectors = [
                'div[data-qa="vacancy-description"]',
                'div.vacancy-description',
                'div.g-user-content',
                'div.b-vacancy-desc',
                'div.vacancy-section'
            ]
            
            for selector in desc_selectors:
                desc_elem = soup.select_one(selector)
                if desc_elem:
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ HTML Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼
                    description_html = str(desc_elem)
                    # Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ
                    description_text = desc_elem.get_text(separator='\n', strip=True)
                    break
            
            if not description_text and json_ld and 'description' in json_ld:
                description_html = json_ld['description']
                soup_desc = BeautifulSoup(description_html, 'html.parser')
                description_text = soup_desc.get_text(separator='\n', strip=True)
            
            # === ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• ĞĞĞ’Ğ«ĞšĞ˜ Ğ˜Ğ— Ğ”ĞĞĞĞ«Ğ¥ Ğ¡Ğ¢Ğ ĞĞĞ˜Ğ¦Ğ« ===
            key_skills = self._extract_key_skills_from_page_data(html_content)
            
            if not key_skills:
                logger.debug(f"No key skills found for vacancy {vacancy_id}")
            else:
                logger.info(f"âœ… Found {len(key_skills)} key skills: {', '.join(key_skills)}")
            
            # === Ğ—ĞĞ ĞŸĞ›ĞĞ¢Ğ ===
            salary_selectors = [
                'span[data-qa="vacancy-salary"]',
                'div[data-qa="vacancy-salary"]',
                'p.vacancy-salary',
                'span.vacancy-salary'
            ]
            
            for selector in salary_selectors:
                salary_elem = soup.select_one(selector)
                if salary_elem:
                    salary = self._parse_salary(salary_elem)
                    break
            
            # === ĞĞŸĞ«Ğ¢ Ğ ĞĞ‘ĞĞ¢Ğ« ===
            experience_id = 'noExperience'
            experience_name = 'ĞĞµÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°'
            
            exp_selectors = [
                'span[data-qa="vacancy-experience"]',
                'p[data-qa="vacancy-experience"]',
                'div.vacancy-experience'
            ]
            
            for selector in exp_selectors:
                exp_elem = soup.select_one(selector)
                if exp_elem:
                    exp_text = exp_elem.get_text(strip=True)
                    parsed_exp = self._parse_experience(exp_text)
                    if parsed_exp[0]:
                        experience_id, experience_name = parsed_exp
                    break
            
            # === Ğ¢Ğ˜ĞŸ Ğ—ĞĞĞ¯Ğ¢ĞĞ¡Ğ¢Ğ˜ ===
            employment_id = 'full'
            employment_name = 'ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ'
            
            emp_selectors = [
                'p[data-qa="vacancy-employment"]',
                'span[data-qa="vacancy-employment"]'
            ]
            
            for selector in emp_selectors:
                emp_elem = soup.select_one(selector)
                if emp_elem:
                    emp_text = emp_elem.get_text(strip=True)
                    parsed_emp = self._parse_employment(emp_text)
                    if parsed_emp[0]:
                        employment_id, employment_name = parsed_emp
                    break
            
            if not employment_id and json_ld and 'employmentType' in json_ld:
                emp_type = json_ld['employmentType']
                if emp_type == 'FULL_TIME':
                    employment_id, employment_name = 'full', 'ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ'
                elif emp_type == 'PART_TIME':
                    employment_id, employment_name = 'part', 'Ğ§Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ'
            
            # === Ğ“Ğ ĞĞ¤Ğ˜Ğš Ğ ĞĞ‘ĞĞ¢Ğ« ===
            schedule_id = None
            schedule_name = None
            
            schedule_selectors = [
                'p[data-qa="vacancy-schedule"]',
                'span[data-qa="vacancy-schedule"]'
            ]
            
            for selector in schedule_selectors:
                schedule_elem = soup.select_one(selector)
                if schedule_elem:
                    schedule_text = schedule_elem.get_text(strip=True)
                    parsed_schedule = self._parse_schedule(schedule_text)
                    if parsed_schedule[0]:
                        schedule_id, schedule_name = parsed_schedule
                    break
            
            # === ĞĞ”Ğ Ğ•Ğ¡ Ğ˜ ĞšĞĞĞ Ğ”Ğ˜ĞĞĞ¢Ğ« ===
            address_raw = None
            address_selectors = [
                'span[data-qa="vacancy-view-raw-address"]',
                'div[data-qa="vacancy-address"]',
                'p[data-qa="vacancy-view-location"]',
                'span.vacancy-address-text'
            ]
            
            for selector in address_selectors:
                address_elem = soup.select_one(selector)
                if address_elem:
                    address_raw = address_elem.get_text(strip=True)
                    break
            
            if not address_raw and json_ld and 'jobLocation' in json_ld:
                location = json_ld['jobLocation']
                if 'address' in location:
                    addr = location['address']
                    if isinstance(addr, dict):
                        parts = []
                        if addr.get('addressLocality'):
                            parts.append(addr.get('addressLocality'))
                        if addr.get('streetAddress'):
                            parts.append(addr.get('streetAddress'))
                        address_raw = ', '.join(parts)
                    else:
                        address_raw = str(addr)
            
            # Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ°Ğ´Ñ€ĞµÑĞ°
            if address_raw:
                parsed_address = self._parse_complex_address(address_raw)
                address.city = parsed_address['city']
                address.street = parsed_address['street']
                address.building = parsed_address['building']
                address.metro_stations = parsed_address['metro_stations']
                address.raw = address_raw
                
                logger.debug(f"Parsed address: city={address.city}, metro={address.metro_stations}, street={address.street}, building={address.building}")
            
            # === Ğ˜Ğ—Ğ’Ğ›Ğ•ĞšĞĞ•Ğœ ĞšĞĞĞ Ğ”Ğ˜ĞĞĞ¢Ğ« Ğ”Ğ›Ğ¯ ĞšĞĞ Ğ¢Ğ« ===
            lat, lng = self._extract_coordinates(soup)
            if lat and lng:
                address.lat = lat
                address.lng = lng
                logger.info(f"Found coordinates: {lat}, {lng}")
            
            # === Ğ¯Ğ—Ğ«ĞšĞ˜ ===
            languages = []
            lang_container = soup.find(['div', 'p'], text=re.compile(r'Ğ—Ğ½Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²|Languages', re.I))
            if lang_container and lang_container.parent:
                lang_elements = lang_container.parent.find_all(['p', 'li'])
                for lang_elem in lang_elements:
                    lang_text = lang_elem.get_text(strip=True)
                    if lang_text and 'Ğ—Ğ½Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²' not in lang_text and 'Languages' not in lang_text:
                        parts = re.split(r'[-â€”â€“]', lang_text)
                        if len(parts) >= 2:
                            languages.append({
                                'name': parts[0].strip(),
                                'level': parts[1].strip()
                            })
                        elif lang_text:
                            languages.append({
                                'name': lang_text.strip(),
                                'level': None
                            })
            
            # === Ğ”ĞĞ¢Ğ ĞŸĞ£Ğ‘Ğ›Ğ˜ĞšĞĞ¦Ğ˜Ğ˜ ===
            published_at = None
            date_selectors = [
                'p[data-qa="vacancy-creation-time"]',
                'p[data-qa="vacancy-view-creation-date"]',
                'span.vacancy-creation-time'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    date_text = re.sub(r'([Ğ Ñ€]Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¾|[ĞĞ¾]Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾|[Ğ’Ğ²]Ğ°ĞºĞ°Ğ½ÑĞ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ°)\s*', '', date_text).strip()
                    published_at = date_text
                    break
            
            if not published_at and json_ld and 'datePosted' in json_ld:
                published_at = json_ld['datePosted']
            
            # === Ğ¡ĞĞ—Ğ”ĞĞ•Ğœ ĞĞ‘ĞªĞ•ĞšĞ¢ Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ˜ ===
            vacancy = Vacancy(
                id=vacancy_id,
                name=name,
                url=url,
                employer=employer,
                published_at=published_at,
                created_at=datetime.now().isoformat(),
                description_html=description_html,
                description_text=description_text,
                key_skills=key_skills,
                salary=salary,
                experience_id=experience_id,
                experience_name=experience_name,
                schedule_id=schedule_id,
                schedule_name=schedule_name,
                employment_id=employment_id,
                employment_name=employment_name,
                address=address,
                languages=languages
            )
            
            # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ, ÑƒĞ±Ğ¸Ñ€Ğ°Ñ None Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑĞ¿Ğ¸ÑĞºĞ¸
            result = asdict(vacancy)
            result = self._clean_dict(result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error parsing vacancy page {url}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _clean_dict(self, d: Dict) -> Dict:
        """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¾Ñ‚ None Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿ÑƒÑÑ‚Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€"""
        if not isinstance(d, dict):
            return d
        
        cleaned = {}
        for key, value in d.items():
            if value is not None:
                if isinstance(value, dict):
                    cleaned_value = self._clean_dict(value)
                    if cleaned_value:
                        cleaned[key] = cleaned_value
                elif isinstance(value, list):
                    if value:
                        cleaned_list = []
                        for item in value:
                            if isinstance(item, dict):
                                cleaned_item = self._clean_dict(item)
                                if cleaned_item:
                                    cleaned_list.append(cleaned_item)
                            else:
                                cleaned_list.append(item)
                        if cleaned_list:
                            cleaned[key] = cleaned_list
                elif value != "":
                    cleaned[key] = value
        
        return cleaned
    
    def save_to_json(self, vacancies: List[Dict], filename: str = "rabota_vacancies.json"):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ğ² JSON Ñ„Ğ°Ğ¹Ğ»"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'meta': {
                        'source': 'rabota.by',
                        'parsed_at': datetime.now().isoformat(),
                        'total_vacancies': len(vacancies)
                    },
                    'vacancies': vacancies
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Saved {len(vacancies)} vacancies to {filename}")
            return True
        except Exception as e:
            logger.error(f"âŒ Error saving to JSON: {e}")
            return False
    
    def close(self):
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµÑÑĞ¸Ğ¸"""
        if self.session:
            self.session.close()


def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""
    print("=" * 60)
    print("ğŸš€ Starting rabota.by parser (final version)")
    print("=" * 60)
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€ÑĞµÑ€
    parser = RabotaByParser()
    
    try:
        # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸
        print("\nğŸ“‹ Fetching vacancies...")
        vacancies = parser.parse_vacancy_list(
            search_text="Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚",
            area="16"  # Ğ‘ĞµĞ»Ğ°Ñ€ÑƒÑÑŒ
        )
        
        if vacancies:
            print(f"\nâœ… Successfully parsed {len(vacancies)} vacancies")
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² JSON
            parser.save_to_json(vacancies, "rabota_vacancies.json")
            
            # === Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ===
            print("\n" + "=" * 60)
            print("ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ĞŸĞĞ Ğ¡Ğ˜ĞĞ“Ğ")
            print("=" * 60)
            
            # ĞŸĞ¾Ğ´ÑÑ‡ĞµÑ‚ Ğ¿Ğ¾Ğ»ĞµĞ¹
            fields_stats = {
                'ğŸ’° Ğ¡ Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ğ¹': 0,
                'ğŸ¯ Ğ¡ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸': 0,
                'ğŸ“ Ğ¡ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸': 0,
                'ğŸš‡ Ğ¡ Ğ¼ĞµÑ‚Ñ€Ğ¾': 0,
                'ğŸŒ Ğ¡ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸': 0,
                'ğŸ‘” Ğ¡ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼': 0,
                'ğŸ¢ Ğ¡ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¼': 0,
                'ğŸ“ Ğ¡ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼': 0
            }
            
            for v in vacancies:
                if v.get('salary'):
                    fields_stats['ğŸ’° Ğ¡ Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ¾Ğ¹'] += 1
                if v.get('key_skills'):
                    fields_stats['ğŸ¯ Ğ¡ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸'] += 1
                if v.get('address', {}).get('lat'):
                    fields_stats['ğŸ“ Ğ¡ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸'] += 1
                if v.get('address', {}).get('metro_stations'):
                    fields_stats['ğŸš‡ Ğ¡ Ğ¼ĞµÑ‚Ñ€Ğ¾'] += 1
                if v.get('languages'):
                    fields_stats['ğŸŒ Ğ¡ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸'] += 1
                if v.get('experience_id'):
                    fields_stats['ğŸ‘” Ğ¡ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼'] += 1
                if v.get('employer', {}).get('name'):
                    fields_stats['ğŸ¢ Ğ¡ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¼'] += 1
                if v.get('description_text'):
                    fields_stats['ğŸ“ Ğ¡ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼'] += 1
            
            for field, count in fields_stats.items():
                percentage = (count * 100) // len(vacancies) if len(vacancies) > 0 else 0
                bar = 'â–ˆ' * (percentage // 10) + 'â–‘' * (10 - percentage // 10)
                print(f"{field:25} [{bar}] {count}/{len(vacancies)} ({percentage}%)")
            
            # === ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ™ ===
            print("\n" + "=" * 60)
            print("ğŸ“„ ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ’ĞĞšĞĞĞ¡Ğ˜Ğ™")
            print("=" * 60)
            
            for i, v in enumerate(vacancies[:3], 1):
                print(f"\n{i}. {v.get('name', 'Ğ‘ĞµĞ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ')}")
                print("-" * 40)
                
                if v.get('employer', {}).get('name'):
                    trusted = "âœ…" if v.get('employer', {}).get('trusted') else ""
                    print(f"   ğŸ¢ ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ: {v['employer']['name']} {trusted}")
                
                if v.get('salary'):
                    sal = v['salary']
                    salary_str = ""
                    if sal.get('from_value'):
                        salary_str = f"Ğ¾Ñ‚ {sal['from_value']:,}"
                    if sal.get('to_value'):
                        if salary_str:
                            salary_str += f" Ğ´Ğ¾ {sal['to_value']:,}"
                        else:
                            salary_str = f"Ğ´Ğ¾ {sal['to_value']:,}"
                    salary_str += f" {sal.get('currency', 'BYN')}"
                    if sal.get('gross'):
                        salary_str += " (gross)"
                    print(f"   ğŸ’° Ğ—Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ°: {salary_str}")
                
                print(f"   ğŸ‘” ĞĞ¿Ñ‹Ñ‚: {v.get('experience_name', 'ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½')}")
                print(f"   ğŸ“… Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº: {v.get('schedule_name', 'ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½')}")
                
                if v.get('key_skills'):
                    skills_preview = v['key_skills'][:5]
                    if len(v['key_skills']) > 5:
                        skills_preview.append(f"... +{len(v['key_skills']) - 5}")
                    print(f"   ğŸ¯ ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸: {', '.join(skills_preview)}")
                
                if v.get('address'):
                    addr = v['address']
                    addr_parts = []
                    if addr.get('city'):
                        addr_parts.append(f"Ğ³. {addr['city']}")
                    if addr.get('metro_stations'):
                        addr_parts.append(f"Ğ¼. {', '.join(addr['metro_stations'])}")
                    if addr.get('street'):
                        addr_parts.append(addr['street'])
                    if addr.get('building'):
                        addr_parts.append(addr['building'])
                    
                    if addr_parts:
                        print(f"   ğŸ“ ĞĞ´Ñ€ĞµÑ: {', '.join(addr_parts)}")
                    
                    if addr.get('lat') and addr.get('lng'):
                        print(f"   ğŸ—ºï¸  ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹: {addr['lat']:.6f}, {addr['lng']:.6f}")
                
                if v.get('languages'):
                    langs = [f"{l['name']}" + (f" ({l['level']})" if l.get('level') else "") 
                            for l in v['languages']]
                    print(f"   ğŸŒ Ğ¯Ğ·Ñ‹ĞºĞ¸: {', '.join(langs)}")
                
                print(f"   ğŸ”— URL: {v.get('url', '')}")
        else:
            print("\nâŒ Ğ’Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
            
    except Exception as e:
        print(f"\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        import traceback
        traceback.print_exc()
    finally:
        parser.close()
        print("\n" + "=" * 60)
        print("âœ… ĞŸĞ°Ñ€ÑĞµÑ€ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ» Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ")
        print("=" * 60)


if __name__ == "__main__":
    main()